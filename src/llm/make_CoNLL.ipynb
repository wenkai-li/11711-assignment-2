{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "t = open('predict.txt').read()\n",
    "# print(t)\n",
    "t = re.sub(r' O(?!\\n)', r' O\\n', t)\n",
    "t = re.sub(r' B-TaskName(?!\\n)', r' B-TaskName\\n', t)\n",
    "t = re.sub(r' B-DatasetName(?!\\n)', r' B-DatasetName\\n', t)\n",
    "t = re.sub(r' B-HyperparameterName(?!\\n)', r' B-HyperparameterName\\n', t)\n",
    "t = re.sub(r' B-HyperparameterValue(?!\\n)', r' B-HyperparameterValue\\n', t)\n",
    "t = re.sub(r' B-MetricValue(?!\\n)', r' B-MetricValue\\n', t)\n",
    "t = re.sub(r' B-MetricName(?!\\n)', r' B-MetricName\\n', t)\n",
    "t = re.sub(r' B-MethodName(?!\\n)', r' B-MethodName\\n', t)\n",
    "\n",
    "fd = open('match.txt','w')\n",
    "\n",
    "print(t,file=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Model's Predictions): 0.8979\n",
      "F1 Score (All 'O' Predictions): 0.9425\n",
      "Accuracy (Model's Predictions): 0.8979\n",
      "Accuracy (All 'O' Predictions): 0.9425\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "        b-datasetname       0.05      0.29      0.09        14\n",
      " b-hyperparametername       0.00      0.00      0.00         2\n",
      "b-hyperparametervalue       0.00      0.00      0.00         5\n",
      "         b-methodname       0.79      0.23      0.36       133\n",
      "         b-metricname       0.29      0.22      0.25         9\n",
      "        b-metricvalue       0.62      0.71      0.67         7\n",
      "           b-taskname       0.07      0.57      0.13        14\n",
      "        i-datasetname       0.00      0.00      0.00        32\n",
      " i-hyperparametername       0.00      0.00      0.00         4\n",
      "i-hyperparametervalue       0.00      0.00      0.00         1\n",
      "         i-methodname       0.00      0.00      0.00        86\n",
      "         i-metricname       0.33      0.15      0.21        13\n",
      "        i-metricvalue       0.00      0.00      0.00         6\n",
      "           i-taskname       0.10      0.45      0.16        33\n",
      "                    o       0.96      0.94      0.95      5887\n",
      "\n",
      "             accuracy                           0.90      6246\n",
      "            macro avg       0.21      0.24      0.19      6246\n",
      "         weighted avg       0.92      0.90      0.90      6246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zw3/miniconda3/envs/llama2/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zw3/miniconda3/envs/llama2/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zw3/miniconda3/envs/llama2/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    original_list = [line.strip().lower() for line in lines]\n",
    "    filtered_list = [item for item in original_list if item.strip()]\n",
    "    return filtered_list\n",
    "\n",
    "# Replace 'predictions.txt' and 'gold_labels.txt' with the file paths of your prediction and gold label files.\n",
    "prediction_file = 'match.txt'\n",
    "gold_label_file = 'test.conll'\n",
    "\n",
    "predictions = read_file(prediction_file)\n",
    "gold_labels = read_file(gold_label_file)\n",
    "\n",
    "# Convert labels to lowercase and split into tokens\n",
    "predictions = [line.split()[1] for line in predictions]\n",
    "gold_labels = [line.split()[1] for line in gold_labels]\n",
    "\n",
    "# Calculate F1 score for the model's predictions\n",
    "model_f1 = f1_score(gold_labels, predictions, average='micro')\n",
    "model_acc = accuracy_score(gold_labels, predictions)\n",
    "# Create a list of \"O\" labels with the same length as predictions\n",
    "all_o_labels = [\"o\"] * len(predictions)\n",
    "\n",
    "# Calculate F1 score for the case where all predictions are \"O\"\n",
    "all_o_f1 = f1_score(gold_labels, all_o_labels, average='micro')\n",
    "all_o_acc = accuracy_score(gold_labels, all_o_labels)\n",
    "\n",
    "\n",
    "print(f\"F1 Score (Model's Predictions): {model_f1:.4f}\")\n",
    "print(f\"F1 Score (All 'O' Predictions): {all_o_f1:.4f}\")\n",
    "print(f\"Accuracy (Model's Predictions): {model_acc:.4f}\")\n",
    "print(f\"Accuracy (All 'O' Predictions): {all_o_acc:.4f}\")\n",
    "print(classification_report(gold_labels,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
