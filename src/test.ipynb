{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(file_path):\n",
    "    sentences = []\n",
    "    ner_tags = []\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    all_tags = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.strip().split()\n",
    "            if len(split_line) > 1:    \n",
    "                tokens.append(split_line[0])\n",
    "                tags.append(split_line[-1])\n",
    "                all_tags.append(split_line[-1])\n",
    "                if split_line[0] in {'.', '!', '?'}:\n",
    "                    sentences.append(tokens)\n",
    "                    ner_tags.append(tags)\n",
    "                    tokens, tags = [], []  # Reset for next sentence\n",
    "    return {\"tokens\": sentences, \"ner_tags\": ner_tags}, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (257,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/wenkaili/python_project/ANLP/Roberta Github/src/test.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m  \u001b[39m# 80% for training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m valid_test_size \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m  \u001b[39m# 10% for validation and 10% for testing\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_data, temp_data, train_tags, temp_tags \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     dataset[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m], dataset[\u001b[39m'\u001b[39;49m\u001b[39mner_tags\u001b[39;49m\u001b[39m'\u001b[39;49m], train_size\u001b[39m=\u001b[39;49mtrain_size, stratify\u001b[39m=\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mner_tags\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m valid_data, test_data, valid_tags, test_tags \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     temp_data, temp_tags, train_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, stratify\u001b[39m=\u001b[39mtemp_tags  \u001b[39m# Split the remaining data equally between validation and test\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wenkaili/python_project/ANLP/Roberta%20Github/src/test.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2638\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2634\u001b[0m         CVClass \u001b[39m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2636\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2638\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39;49msplit(X\u001b[39m=\u001b[39;49marrays[\u001b[39m0\u001b[39;49m], y\u001b[39m=\u001b[39;49mstratify))\n\u001b[1;32m   2640\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2641\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2643\u001b[0m     )\n\u001b[1;32m   2644\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2197\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2164\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \n\u001b[1;32m   2166\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[39m    to an integer.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2197\u001b[0m     y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (257,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "dataset, all_tags= load_custom_dataset(\"output.conll\")\n",
    "dataset['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DatasetName',\n",
       " 'B-HyperparameterName',\n",
       " 'B-HyperparameterValue',\n",
       " 'B-MethodName',\n",
       " 'B-MetricName',\n",
       " 'B-MetricValue',\n",
       " 'B-TaskName',\n",
       " 'I-DatasetName',\n",
       " 'I-HyperparameterName',\n",
       " 'I-HyperparameterValue',\n",
       " 'I-MethodName',\n",
       " 'I-MetricName',\n",
       " 'I-MetricValue',\n",
       " 'I-TaskName',\n",
       " 'O']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_tags).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From\n",
      "Pretraining\n",
      "Data\n",
      "to\n",
      "Language\n",
      "Models\n",
      "to\n",
      "Downstream\n",
      "Tasks\n",
      ":\n",
      "Tracking\n",
      "the\n",
      "Trails\n",
      "of\n",
      "Political\n",
      "Biases\n",
      "Leading\n",
      "to\n",
      "Unfair\n",
      "NLP\n",
      "Models\n",
      "Language\n",
      "models\n",
      "(\n",
      "LMs\n",
      ")\n",
      "are\n",
      "pretrained\n",
      "on\n",
      "diverse\n",
      "data\n",
      "sources\n",
      ",\n",
      "including\n",
      "news\n",
      ",\n",
      "discussion\n",
      "forums\n",
      ",\n",
      "books\n",
      ",\n",
      "and\n",
      "online\n",
      "encyclopedias\n",
      ".\n",
      "A\n",
      "significant\n",
      "portion\n",
      "of\n",
      "this\n",
      "data\n",
      "includes\n",
      "opinions\n",
      "and\n",
      "perspectives\n",
      "which\n",
      ",\n",
      "on\n",
      "one\n",
      "hand\n",
      ",\n",
      "celebrate\n",
      "democracy\n",
      "and\n",
      "diversity\n",
      "of\n",
      "ideas\n",
      ",\n",
      "and\n",
      "on\n",
      "the\n",
      "other\n",
      "hand\n",
      "are\n",
      "inherently\n",
      "socially\n",
      "biased\n",
      ".\n",
      "Our\n",
      "work\n",
      "develops\n",
      "new\n",
      "methods\n",
      "to\n",
      "(\n",
      "1\n",
      ")\n",
      "measure\n",
      "political\n",
      "biases\n",
      "in\n",
      "LMs\n",
      "trained\n",
      "on\n",
      "such\n",
      "corpora\n",
      ",\n",
      "along\n",
      "social\n",
      "and\n",
      "economic\n",
      "axes\n",
      ",\n",
      "and\n",
      "(\n",
      "2\n",
      ")\n",
      "measure\n",
      "the\n",
      "fairness\n",
      "of\n",
      "downstream\n",
      "NLP\n",
      "models\n",
      "trained\n",
      "on\n",
      "top\n",
      "of\n",
      "politically\n",
      "biased\n",
      "LMs\n",
      ".\n",
      "We\n",
      "focus\n",
      "on\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "detection\n",
      ",\n",
      "aiming\n",
      "to\n",
      "empirically\n",
      "quantify\n",
      "the\n",
      "effects\n",
      "of\n",
      "political\n",
      "(\n",
      "social\n",
      ",\n",
      "economic\n",
      ")\n",
      "biases\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "on\n",
      "the\n",
      "fairness\n",
      "of\n",
      "high\n",
      "-\n",
      "stakes\n",
      "social\n",
      "-\n",
      "oriented\n",
      "tasks\n",
      ".\n",
      "Our\n",
      "findings\n",
      "reveal\n",
      "that\n",
      "pretrained\n",
      "LMs\n",
      "do\n",
      "have\n",
      "political\n",
      "leanings\n",
      "that\n",
      "reinforce\n",
      "the\n",
      "polarization\n",
      "present\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      ",\n",
      "propagating\n",
      "social\n",
      "biases\n",
      "into\n",
      "hate\n",
      "speech\n",
      "predictions\n",
      "and\n",
      "misinformation\n",
      "detectors\n",
      ".\n",
      "We\n",
      "discuss\n",
      "the\n",
      "implications\n",
      "of\n",
      "our\n",
      "findings\n",
      "for\n",
      "NLP\n",
      "research\n",
      "and\n",
      "propose\n",
      "future\n",
      "directions\n",
      "to\n",
      "mitigate\n",
      "unfairness\n",
      ".\n",
      "1\n",
      "Warning\n",
      ":\n",
      "This\n",
      "paper\n",
      "contains\n",
      "examples\n",
      "of\n",
      "hate\n",
      "speech\n",
      ".\n",
      "Introduction\n",
      "Digital\n",
      "and\n",
      "social\n",
      "media\n",
      "have\n",
      "become\n",
      "a\n",
      "major\n",
      "source\n",
      "of\n",
      "political\n",
      "news\n",
      "dissemination\n",
      "(\n",
      "Hermida\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2012\n",
      ";\n",
      "Kümpel\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2015\n",
      ";\n",
      "Hermida\n",
      ",\n",
      "2016\n",
      ")\n",
      "with\n",
      "unprecedentedly\n",
      "high\n",
      "user\n",
      "engagement\n",
      "rates\n",
      "(\n",
      "Mustafaraj\n",
      "and\n",
      "Metaxas\n",
      ",\n",
      "2011\n",
      ";\n",
      "Velasquez\n",
      ",\n",
      "2012\n",
      ";\n",
      "Garimella\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ")\n",
      ".\n",
      "The\n",
      "volume\n",
      "of\n",
      "online\n",
      "discourse\n",
      "surrounding\n",
      "polarizing\n",
      "issues\n",
      "-\n",
      "climate\n",
      "change\n",
      ",\n",
      "gun\n",
      "control\n",
      ",\n",
      "abortion\n",
      ",\n",
      "wage\n",
      "gaps\n",
      ",\n",
      "death\n",
      "penalty\n",
      ",\n",
      "taxes\n",
      ",\n",
      "same\n",
      "-\n",
      "sex\n",
      "marriage\n",
      ",\n",
      "and\n",
      "more\n",
      "-\n",
      "has\n",
      "been\n",
      "drastically\n",
      "growing\n",
      "in\n",
      "the\n",
      "past\n",
      "decade\n",
      "(\n",
      "Valenzuela\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2012\n",
      ";\n",
      "Rainie\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2012\n",
      ";\n",
      "Enikolopov\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ".\n",
      "While\n",
      "online\n",
      "political\n",
      "engagement\n",
      "promotes\n",
      "democratic\n",
      "values\n",
      "and\n",
      "diversity\n",
      "of\n",
      "perspectives\n",
      ",\n",
      "these\n",
      "discussions\n",
      "also\n",
      "reflect\n",
      "and\n",
      "reinforce\n",
      "societal\n",
      "biases\n",
      "-\n",
      "stereotypical\n",
      "generalizations\n",
      "about\n",
      "people\n",
      "or\n",
      "social\n",
      "groups\n",
      "(\n",
      "Devine\n",
      ",\n",
      "1989\n",
      ";\n",
      "Bargh\n",
      ",\n",
      "1999\n",
      ";\n",
      "Blair\n",
      ",\n",
      "2002\n",
      ")\n",
      ".\n",
      "Such\n",
      "language\n",
      "constitutes\n",
      "a\n",
      "major\n",
      "portion\n",
      "of\n",
      "large\n",
      "language\n",
      "models\n",
      "'\n",
      "(\n",
      "LMs\n",
      ")\n",
      "pretraining\n",
      "data\n",
      ",\n",
      "propagating\n",
      "biases\n",
      "into\n",
      "downstream\n",
      "models\n",
      ".\n",
      "Hundreds\n",
      "of\n",
      "studies\n",
      "have\n",
      "highlighted\n",
      "ethical\n",
      "issues\n",
      "in\n",
      "NLP\n",
      "models\n",
      "(\n",
      "Blodgett\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020a\n",
      ";\n",
      "Field\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Kumar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      "and\n",
      "designed\n",
      "synthetic\n",
      "datasets\n",
      "(\n",
      "Nangia\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ";\n",
      "Nadeem\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      "or\n",
      "controlled\n",
      "experiments\n",
      "to\n",
      "measure\n",
      "how\n",
      "biases\n",
      "in\n",
      "language\n",
      "are\n",
      "encoded\n",
      "in\n",
      "learned\n",
      "representations\n",
      "(\n",
      "Sun\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "and\n",
      "how\n",
      "annotator\n",
      "errors\n",
      "in\n",
      "training\n",
      "data\n",
      "are\n",
      "liable\n",
      "to\n",
      "increase\n",
      "unfairness\n",
      "of\n",
      "NLP\n",
      "models\n",
      "(\n",
      "Sap\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ".\n",
      "However\n",
      ",\n",
      "the\n",
      "language\n",
      "of\n",
      "polarizing\n",
      "political\n",
      "issues\n",
      "is\n",
      "particularly\n",
      "complex\n",
      "(\n",
      "Demszky\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "and\n",
      "social\n",
      "biases\n",
      "hidden\n",
      "in\n",
      "language\n",
      "can\n",
      "rarely\n",
      "be\n",
      "reduced\n",
      "to\n",
      "pre\n",
      "-\n",
      "specified\n",
      "stereotypical\n",
      "associations\n",
      "(\n",
      "Joseph\n",
      "and\n",
      "Morgan\n",
      ",\n",
      "2020\n",
      ")\n",
      ".\n",
      "To\n",
      "the\n",
      "best\n",
      "of\n",
      "our\n",
      "knowledge\n",
      ",\n",
      "no\n",
      "prior\n",
      "work\n",
      "has\n",
      "shown\n",
      "how\n",
      "to\n",
      "analyze\n",
      "the\n",
      "effects\n",
      "of\n",
      "naturally\n",
      "occurring\n",
      "media\n",
      "biases\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "on\n",
      "language\n",
      "models\n",
      ",\n",
      "and\n",
      "subsequently\n",
      "on\n",
      "downstream\n",
      "tasks\n",
      ",\n",
      "and\n",
      "how\n",
      "it\n",
      "affects\n",
      "the\n",
      "fairness\n",
      "towards\n",
      "diverse\n",
      "social\n",
      "groups\n",
      ".\n",
      "Our\n",
      "study\n",
      "aims\n",
      "to\n",
      "fill\n",
      "this\n",
      "gap\n",
      ".\n",
      "As\n",
      "a\n",
      "case\n",
      "study\n",
      ",\n",
      "we\n",
      "focus\n",
      "on\n",
      "the\n",
      "effects\n",
      "of\n",
      "media\n",
      "biases\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "on\n",
      "the\n",
      "fairness\n",
      "of\n",
      "hate\n",
      "speech\n",
      "detection\n",
      "with\n",
      "respect\n",
      "to\n",
      "diverse\n",
      "social\n",
      "attributes\n",
      ",\n",
      "such\n",
      "as\n",
      "gender\n",
      ",\n",
      "race\n",
      ",\n",
      "ethnicity\n",
      ",\n",
      "religion\n",
      ",\n",
      "and\n",
      "sexual\n",
      "orientation\n",
      ",\n",
      "and\n",
      "of\n",
      "misinformation\n",
      "detection\n",
      "with\n",
      "respect\n",
      "to\n",
      "partisan\n",
      "leanings\n",
      ".\n",
      "We\n",
      "investigate\n",
      "how\n",
      "media\n",
      "biases\n",
      "in\n",
      "the\n",
      "pretraining\n",
      "data\n",
      "propagate\n",
      "into\n",
      "LMs\n",
      "and\n",
      "ultimately\n",
      "affect\n",
      "downstream\n",
      "tasks\n",
      ",\n",
      "because\n",
      "discussions\n",
      "about\n",
      "polarizing\n",
      "social\n",
      "and\n",
      "economic\n",
      "issues\n",
      "are\n",
      "abundant\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "sourced\n",
      "from\n",
      "news\n",
      ",\n",
      "forums\n",
      ",\n",
      "books\n",
      ",\n",
      "and\n",
      "online\n",
      "encyclopedias\n",
      ",\n",
      "and\n",
      "this\n",
      "language\n",
      "inevitably\n",
      "perpetuates\n",
      "social\n",
      "stereotypes\n",
      ".\n",
      "We\n",
      "choose\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "classification\n",
      "because\n",
      "these\n",
      "are\n",
      "social\n",
      "-\n",
      "oriented\n",
      "tasks\n",
      "in\n",
      "which\n",
      "unfair\n",
      "predictions\n",
      "can\n",
      "be\n",
      "especially\n",
      "harmful\n",
      "(\n",
      "Duggan\n",
      ",\n",
      "2017\n",
      ";\n",
      "League\n",
      ",\n",
      "2019League\n",
      ",\n",
      ",\n",
      "2021\n",
      ".\n",
      "To\n",
      "this\n",
      "end\n",
      ",\n",
      "grounded\n",
      "in\n",
      "political\n",
      "spectrum\n",
      "theories\n",
      "(\n",
      "Eysenck\n",
      ",\n",
      "1957\n",
      ";\n",
      "Rokeach\n",
      ",\n",
      "1973\n",
      ";\n",
      "Gindler\n",
      ",\n",
      "2021\n",
      ")\n",
      "and\n",
      "the\n",
      "political\n",
      "compass\n",
      "test\n",
      ",\n",
      "2\n",
      "we\n",
      "propose\n",
      "to\n",
      "empirically\n",
      "quantify\n",
      "the\n",
      "political\n",
      "leaning\n",
      "of\n",
      "pretrained\n",
      "LMs\n",
      "(\n",
      "§\n",
      "2\n",
      ")\n",
      ".\n",
      "We\n",
      "then\n",
      "further\n",
      "pretrain\n",
      "language\n",
      "models\n",
      "on\n",
      "different\n",
      "partisan\n",
      "corpora\n",
      "to\n",
      "investigate\n",
      "whether\n",
      "LMs\n",
      "pick\n",
      "up\n",
      "political\n",
      "biases\n",
      "from\n",
      "training\n",
      "data\n",
      ".\n",
      "Finally\n",
      ",\n",
      "we\n",
      "train\n",
      "classifiers\n",
      "on\n",
      "top\n",
      "of\n",
      "LMs\n",
      "with\n",
      "varying\n",
      "political\n",
      "leanings\n",
      "and\n",
      "evaluate\n",
      "their\n",
      "performance\n",
      "on\n",
      "hate\n",
      "speech\n",
      "instances\n",
      "targeting\n",
      "different\n",
      "identity\n",
      "groups\n",
      "(\n",
      "Yoder\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      ",\n",
      "and\n",
      "on\n",
      "misinformation\n",
      "detection\n",
      "with\n",
      "different\n",
      "agendas\n",
      "(\n",
      "Wang\n",
      ",\n",
      "2017\n",
      ")\n",
      ".\n",
      "In\n",
      "this\n",
      "way\n",
      ",\n",
      "we\n",
      "investigate\n",
      "the\n",
      "propagation\n",
      "of\n",
      "political\n",
      "bias\n",
      "through\n",
      "the\n",
      "entire\n",
      "pipeline\n",
      "from\n",
      "pretraining\n",
      "data\n",
      "to\n",
      "language\n",
      "models\n",
      "to\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "Our\n",
      "experiments\n",
      "across\n",
      "several\n",
      "data\n",
      "domains\n",
      ",\n",
      "partisan\n",
      "news\n",
      "datasets\n",
      ",\n",
      "and\n",
      "LM\n",
      "architectures\n",
      "(\n",
      "§\n",
      "3\n",
      ")\n",
      "demonstrate\n",
      "that\n",
      "different\n",
      "pretrained\n",
      "LMs\n",
      "do\n",
      "have\n",
      "different\n",
      "underlying\n",
      "political\n",
      "leanings\n",
      ",\n",
      "reinforcing\n",
      "the\n",
      "political\n",
      "polarization\n",
      "present\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      "(\n",
      "§\n",
      "4.1\n",
      ")\n",
      ".\n",
      "Further\n",
      ",\n",
      "while\n",
      "the\n",
      "overall\n",
      "performance\n",
      "of\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "detectors\n",
      "remains\n",
      "consistent\n",
      "across\n",
      "such\n",
      "politically\n",
      "-\n",
      "biased\n",
      "LMs\n",
      ",\n",
      "these\n",
      "models\n",
      "exhibit\n",
      "significantly\n",
      "different\n",
      "behaviors\n",
      "against\n",
      "different\n",
      "identity\n",
      "groups\n",
      "and\n",
      "partisan\n",
      "media\n",
      "sources\n",
      ".\n",
      "(\n",
      "§\n",
      "4.2\n",
      ")\n",
      ".\n",
      "The\n",
      "main\n",
      "contributions\n",
      "of\n",
      "this\n",
      "paper\n",
      "are\n",
      "novel\n",
      "methods\n",
      "to\n",
      "quantify\n",
      "political\n",
      "biases\n",
      "in\n",
      "LMs\n",
      ",\n",
      "and\n",
      "findings\n",
      "that\n",
      "shed\n",
      "new\n",
      "light\n",
      "on\n",
      "how\n",
      "ideological\n",
      "polarization\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      "propagates\n",
      "biases\n",
      "into\n",
      "language\n",
      "models\n",
      ",\n",
      "and\n",
      "subsequently\n",
      "into\n",
      "social\n",
      "-\n",
      "oriented\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "In\n",
      "§\n",
      "5\n",
      ",\n",
      "we\n",
      "discuss\n",
      "implications\n",
      "of\n",
      "our\n",
      "findings\n",
      "for\n",
      "NLP\n",
      "research\n",
      ",\n",
      "that\n",
      "no\n",
      "language\n",
      "model\n",
      "can\n",
      "be\n",
      "entirely\n",
      "free\n",
      "from\n",
      "social\n",
      "biases\n",
      ",\n",
      "and\n",
      "propose\n",
      "future\n",
      "directions\n",
      "to\n",
      "mitigate\n",
      "unfairness\n",
      ".\n",
      "Methodology\n",
      "We\n",
      "propose\n",
      "a\n",
      "two\n",
      "-\n",
      "step\n",
      "methodology\n",
      "to\n",
      "establish\n",
      "the\n",
      "effect\n",
      "of\n",
      "political\n",
      "biases\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      "on\n",
      "the\n",
      "fairness\n",
      "of\n",
      "downstream\n",
      "tasks\n",
      ":\n",
      "(\n",
      "1\n",
      ")\n",
      "we\n",
      "develop\n",
      "a\n",
      "framework\n",
      ",\n",
      "grounded\n",
      "in\n",
      "political\n",
      "science\n",
      "literature\n",
      ",\n",
      "to\n",
      "measure\n",
      "the\n",
      "inherent\n",
      "political\n",
      "leanings\n",
      "of\n",
      "pretrained\n",
      "language\n",
      "models\n",
      ",\n",
      "and\n",
      "(\n",
      "2\n",
      ")\n",
      "then\n",
      "investi\n",
      "-\n",
      "gate\n",
      "how\n",
      "the\n",
      "political\n",
      "leanings\n",
      "of\n",
      "LMs\n",
      "affect\n",
      "their\n",
      "performance\n",
      "in\n",
      "downstream\n",
      "social\n",
      "-\n",
      "oriented\n",
      "tasks\n",
      ".\n",
      "Measuring\n",
      "the\n",
      "Political\n",
      "Leanings\n",
      "of\n",
      "LMs\n",
      "While\n",
      "prior\n",
      "works\n",
      "provided\n",
      "analyses\n",
      "of\n",
      "political\n",
      "leanings\n",
      "in\n",
      "LMs\n",
      "(\n",
      "Jiang\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022a\n",
      ";\n",
      "Argyle\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      ",\n",
      "they\n",
      "primarily\n",
      "focused\n",
      "on\n",
      "political\n",
      "individuals\n",
      ",\n",
      "rather\n",
      "than\n",
      "the\n",
      "timeless\n",
      "ideological\n",
      "issues\n",
      "grounded\n",
      "in\n",
      "political\n",
      "science\n",
      "literature\n",
      ".\n",
      "In\n",
      "contrast\n",
      ",\n",
      "our\n",
      "method\n",
      "is\n",
      "grounded\n",
      "in\n",
      "political\n",
      "spectrum\n",
      "theories\n",
      "(\n",
      "Eysenck\n",
      ",\n",
      "1957\n",
      ";\n",
      "Rokeach\n",
      ",\n",
      "1973\n",
      ";\n",
      "Gindler\n",
      ",\n",
      "2021\n",
      ")\n",
      "that\n",
      "provide\n",
      "more\n",
      "nuanced\n",
      "perspective\n",
      "than\n",
      "the\n",
      "commonly\n",
      "used\n",
      "left\n",
      "vs.\n",
      "right\n",
      "distinction\n",
      "(\n",
      "Bobbio\n",
      ",\n",
      "1996\n",
      ";\n",
      "Mair\n",
      ",\n",
      "2007\n",
      ";\n",
      "Corballis\n",
      "and\n",
      "Beale\n",
      ",\n",
      "2020\n",
      ")\n",
      "by\n",
      "assessing\n",
      "political\n",
      "positions\n",
      "on\n",
      "two\n",
      "axes\n",
      ":\n",
      "social\n",
      "values\n",
      "(\n",
      "ranging\n",
      "from\n",
      "liberal\n",
      "to\n",
      "conservative\n",
      ")\n",
      "and\n",
      "economic\n",
      "values\n",
      "(\n",
      "ranging\n",
      "from\n",
      "left\n",
      "to\n",
      "right\n",
      ")\n",
      ".\n",
      "The\n",
      "widely\n",
      "adopted\n",
      "political\n",
      "compass\n",
      "test\n",
      ",\n",
      "2\n",
      "which\n",
      "is\n",
      "based\n",
      "on\n",
      "these\n",
      "theories\n",
      ",\n",
      "measures\n",
      "individuals\n",
      "'\n",
      "leaning\n",
      "on\n",
      "a\n",
      "two\n",
      "-\n",
      "dimensional\n",
      "space\n",
      "by\n",
      "analyzing\n",
      "their\n",
      "responses\n",
      "to\n",
      "62\n",
      "political\n",
      "statements\n",
      ".\n",
      "3\n",
      "Participants\n",
      "indicate\n",
      "their\n",
      "level\n",
      "of\n",
      "agreement\n",
      "or\n",
      "disagreement\n",
      "with\n",
      "each\n",
      "statement\n",
      ",\n",
      "and\n",
      "their\n",
      "responses\n",
      "are\n",
      "used\n",
      "to\n",
      "calculate\n",
      "their\n",
      "social\n",
      "and\n",
      "economic\n",
      "scores\n",
      "through\n",
      "weighted\n",
      "summation\n",
      ".\n",
      "Formally\n",
      ",\n",
      "the\n",
      "political\n",
      "compass\n",
      "test\n",
      "maps\n",
      "a\n",
      "set\n",
      "of\n",
      "answers\n",
      "indicating\n",
      "agreement\n",
      "level\n",
      "{\n",
      "STRONG\n",
      "DISAGREE\n",
      ",\n",
      "DISAGREE\n",
      ",\n",
      "AGREE\n",
      ",\n",
      "STRONG\n",
      "AGREE\n",
      "}\n",
      "62\n",
      "to\n",
      "twodimensional\n",
      "point\n",
      "(\n",
      "s\n",
      "soc\n",
      ",\n",
      "s\n",
      "eco\n",
      ")\n",
      ",\n",
      "where\n",
      "the\n",
      "social\n",
      "score\n",
      "s\n",
      "soc\n",
      "and\n",
      "economic\n",
      "score\n",
      "s\n",
      "eco\n",
      "range\n",
      "from\n",
      "[\n",
      "−10\n",
      ",\n",
      "10\n",
      "]\n",
      ".\n",
      "We\n",
      "employ\n",
      "this\n",
      "test\n",
      "as\n",
      "a\n",
      "tool\n",
      "to\n",
      "measure\n",
      "the\n",
      "political\n",
      "leanings\n",
      "of\n",
      "pretrained\n",
      "language\n",
      "models\n",
      ".\n",
      "We\n",
      "probe\n",
      "a\n",
      "diverse\n",
      "set\n",
      "of\n",
      "LMs\n",
      "to\n",
      "measure\n",
      "their\n",
      "alignment\n",
      "with\n",
      "specific\n",
      "political\n",
      "statements\n",
      ",\n",
      "including\n",
      "encoder\n",
      "and\n",
      "language\n",
      "generation\n",
      "models\n",
      "(\n",
      "decoder\n",
      "and\n",
      "autoregressive\n",
      ")\n",
      ".\n",
      "For\n",
      "encoderonly\n",
      "LMs\n",
      ",\n",
      "we\n",
      "use\n",
      "mask\n",
      "filling\n",
      "with\n",
      "prompts\n",
      "derived\n",
      "from\n",
      "the\n",
      "political\n",
      "statements\n",
      ".\n",
      "We\n",
      "construct\n",
      "the\n",
      "following\n",
      "prompt\n",
      ":\n",
      "\"\n",
      "Please\n",
      "respond\n",
      "to\n",
      "the\n",
      "following\n",
      "statement\n",
      ":\n",
      "[\n",
      "STATEMENT\n",
      "]\n",
      "I\n",
      "<\n",
      "MASK\n",
      ">\n",
      "with\n",
      "this\n",
      "statement\n",
      ".\n",
      "\"\n",
      "Then\n",
      ",\n",
      "pretrained\n",
      "LMs\n",
      "fill\n",
      "the\n",
      "mask\n",
      "and\n",
      "return\n",
      "10\n",
      "highest\n",
      "probability\n",
      "tokens\n",
      ".\n",
      "By\n",
      "comparing\n",
      "the\n",
      "aggregated\n",
      "probability\n",
      "of\n",
      "pre\n",
      "-\n",
      "defined\n",
      "positive\n",
      "(\n",
      "agree\n",
      ",\n",
      "support\n",
      ",\n",
      "endorse\n",
      ",\n",
      "etc\n",
      ".\n",
      ")\n",
      "We\n",
      "probe\n",
      "language\n",
      "generation\n",
      "models\n",
      "by\n",
      "conducting\n",
      "text\n",
      "generation\n",
      "based\n",
      "on\n",
      "the\n",
      "following\n",
      "prompt\n",
      ":\n",
      "\"\n",
      "Please\n",
      "respond\n",
      "to\n",
      "the\n",
      "following\n",
      "statement\n",
      ":\n",
      "[\n",
      "STATEMENT\n",
      "]\n",
      "\\n\n",
      "Your\n",
      "response\n",
      ":\n",
      "\"\n",
      ".\n",
      "We\n",
      "then\n",
      "use\n",
      "an\n",
      "off\n",
      "-\n",
      "the\n",
      "-\n",
      "shelf\n",
      "stance\n",
      "detector\n",
      "to\n",
      "determine\n",
      "whether\n",
      "the\n",
      "generated\n",
      "response\n",
      "agrees\n",
      "or\n",
      "disagrees\n",
      "with\n",
      "the\n",
      "given\n",
      "statement\n",
      ".\n",
      "We\n",
      "use\n",
      "10\n",
      "random\n",
      "seeds\n",
      "for\n",
      "prompted\n",
      "generation\n",
      ",\n",
      "filter\n",
      "low\n",
      "-\n",
      "confidence\n",
      "responses\n",
      "using\n",
      "the\n",
      "stance\n",
      "detector\n",
      ",\n",
      "and\n",
      "average\n",
      "the\n",
      "stance\n",
      "detection\n",
      "scores\n",
      "for\n",
      "a\n",
      "more\n",
      "reliable\n",
      "evaluation\n",
      ".\n",
      "5\n",
      "Using\n",
      "this\n",
      "framework\n",
      ",\n",
      "we\n",
      "aim\n",
      "to\n",
      "systematically\n",
      "evaluate\n",
      "the\n",
      "effect\n",
      "of\n",
      "polarization\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "on\n",
      "the\n",
      "political\n",
      "bias\n",
      "of\n",
      "LMs\n",
      ".\n",
      "We\n",
      "thus\n",
      "train\n",
      "multiple\n",
      "partisan\n",
      "LMs\n",
      "through\n",
      "continued\n",
      "pretraining\n",
      "of\n",
      "existing\n",
      "LMs\n",
      "on\n",
      "data\n",
      "from\n",
      "various\n",
      "political\n",
      "viewpoints\n",
      ",\n",
      "and\n",
      "then\n",
      "evaluate\n",
      "how\n",
      "model\n",
      "'s\n",
      "ideological\n",
      "coordinates\n",
      "shift\n",
      ".\n",
      "In\n",
      "these\n",
      "experiments\n",
      ",\n",
      "we\n",
      "only\n",
      "use\n",
      "established\n",
      "media\n",
      "sources\n",
      ",\n",
      "because\n",
      "our\n",
      "ultimate\n",
      "goal\n",
      "is\n",
      "to\n",
      "understand\n",
      "whether\n",
      "\"\n",
      "clean\n",
      "\"\n",
      "pretraining\n",
      "data\n",
      "(\n",
      "not\n",
      "overtly\n",
      "hateful\n",
      "or\n",
      "toxic\n",
      ")\n",
      "leads\n",
      "to\n",
      "undesirable\n",
      "biases\n",
      "in\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "Measuring\n",
      "the\n",
      "Effect\n",
      "of\n",
      "LM\n",
      "'s\n",
      "Political\n",
      "Bias\n",
      "on\n",
      "Downstream\n",
      "Task\n",
      "Performance\n",
      "Armed\n",
      "with\n",
      "the\n",
      "LM\n",
      "political\n",
      "leaning\n",
      "evaluation\n",
      "framework\n",
      ",\n",
      "we\n",
      "investigate\n",
      "the\n",
      "impact\n",
      "of\n",
      "these\n",
      "biases\n",
      "on\n",
      "downstream\n",
      "tasks\n",
      "with\n",
      "social\n",
      "implications\n",
      "such\n",
      "as\n",
      "hate\n",
      "speech\n",
      "detection\n",
      "and\n",
      "misinformation\n",
      "identification\n",
      ".\n",
      "We\n",
      "fine\n",
      "-\n",
      "tune\n",
      "different\n",
      "partisan\n",
      "versions\n",
      "of\n",
      "the\n",
      "same\n",
      "LM\n",
      "architecture\n",
      "on\n",
      "these\n",
      "tasks\n",
      "and\n",
      "datasets\n",
      "and\n",
      "analyze\n",
      "the\n",
      "results\n",
      "from\n",
      "two\n",
      "perspectives\n",
      ".\n",
      "This\n",
      "is\n",
      "a\n",
      "controlled\n",
      "experiment\n",
      "setting\n",
      ",\n",
      "i.e.\n",
      "only\n",
      "the\n",
      "partisan\n",
      "pretraining\n",
      "corpora\n",
      "is\n",
      "different\n",
      ",\n",
      "while\n",
      "the\n",
      "starting\n",
      "LM\n",
      "checkpoint\n",
      ",\n",
      "task\n",
      "-\n",
      "specific\n",
      "fine\n",
      "-\n",
      "tuning\n",
      "data\n",
      ",\n",
      "and\n",
      "all\n",
      "hyperparameters\n",
      "are\n",
      "the\n",
      "same\n",
      ".\n",
      "First\n",
      ",\n",
      "we\n",
      "look\n",
      "at\n",
      "overall\n",
      "performance\n",
      "differences\n",
      "across\n",
      "LMs\n",
      "with\n",
      "different\n",
      "leanings\n",
      ".\n",
      "Second\n",
      ",\n",
      "we\n",
      "examine\n",
      "per\n",
      "-\n",
      "category\n",
      "performance\n",
      ",\n",
      "breaking\n",
      "down\n",
      "the\n",
      "datasets\n",
      "into\n",
      "different\n",
      "socially\n",
      "informed\n",
      "groups\n",
      "(\n",
      "identity\n",
      "groups\n",
      "for\n",
      "hate\n",
      "speech\n",
      "and\n",
      "media\n",
      "sources\n",
      "for\n",
      "misinformation\n",
      ")\n",
      ",\n",
      "to\n",
      "determine\n",
      "if\n",
      "the\n",
      "inherent\n",
      "political\n",
      "bias\n",
      "in\n",
      "LMs\n",
      "could\n",
      "lead\n",
      "to\n",
      "unfairness\n",
      "in\n",
      "downstream\n",
      "applications\n",
      ".\n",
      "Experiment\n",
      "Settings\n",
      "LM\n",
      "and\n",
      "Stance\n",
      "Detection\n",
      "Model\n",
      "We\n",
      "evaluate\n",
      "political\n",
      "biases\n",
      "of\n",
      "14\n",
      "language\n",
      "models\n",
      ":\n",
      "BERT\n",
      "(\n",
      "Devlin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "RoBERTa\n",
      ",\n",
      "dis\n",
      "-\n",
      "tilBERT\n",
      "(\n",
      "Sanh\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "distilRoBERTa\n",
      ",\n",
      "AL\n",
      "-\n",
      "BERT\n",
      "(\n",
      "Lan\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "BART\n",
      "(\n",
      "Lewis\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ")\n",
      ",\n",
      "GPT-2\n",
      "(\n",
      "Radford\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "GPT-3\n",
      "(\n",
      "Brown\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ")\n",
      ",\n",
      "GPT\n",
      "-\n",
      "J\n",
      "(\n",
      "Wang\n",
      "and\n",
      "Komatsuzaki\n",
      ",\n",
      "2021\n",
      ")\n",
      ",\n",
      "LLaMA\n",
      "(\n",
      "Touvron\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      ")\n",
      ",\n",
      "Alpaca\n",
      "(\n",
      "Taori\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2023\n",
      ")\n",
      ",\n",
      "Codex\n",
      "(\n",
      "Chen\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      ",\n",
      "ChatGPT\n",
      ",\n",
      "GPT-4\n",
      "(\n",
      "OpenAI\n",
      ",\n",
      "2023\n",
      ")\n",
      "and\n",
      "their\n",
      "variants\n",
      ",\n",
      "representing\n",
      "a\n",
      "diverse\n",
      "range\n",
      "of\n",
      "model\n",
      "sizes\n",
      "and\n",
      "architectures\n",
      ".\n",
      "The\n",
      "specific\n",
      "versions\n",
      "and\n",
      "checkpoint\n",
      "names\n",
      "of\n",
      "each\n",
      "model\n",
      "are\n",
      "provided\n",
      "in\n",
      "Appendix\n",
      "C.\n",
      "For\n",
      "the\n",
      "stance\n",
      "detection\n",
      "model\n",
      "used\n",
      "for\n",
      "evaluating\n",
      "decoder\n",
      "-\n",
      "based\n",
      "language\n",
      "model\n",
      "responses\n",
      ",\n",
      "we\n",
      "use\n",
      "a\n",
      "BART\n",
      "-\n",
      "based\n",
      "model\n",
      "trained\n",
      "on\n",
      "MultiNLI\n",
      "(\n",
      "Williams\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ")\n",
      ".\n",
      "To\n",
      "ensure\n",
      "the\n",
      "reliability\n",
      "of\n",
      "the\n",
      "off\n",
      "-\n",
      "the\n",
      "-\n",
      "shelf\n",
      "stance\n",
      "detector\n",
      ",\n",
      "we\n",
      "conduct\n",
      "a\n",
      "human\n",
      "evaluation\n",
      "on\n",
      "110\n",
      "randomly\n",
      "sampled\n",
      "responses\n",
      "and\n",
      "compare\n",
      "the\n",
      "results\n",
      "to\n",
      "those\n",
      "generated\n",
      "by\n",
      "the\n",
      "detector\n",
      ".\n",
      "The\n",
      "stance\n",
      "detector\n",
      "has\n",
      "an\n",
      "accuracy\n",
      "of\n",
      "0.97\n",
      "for\n",
      "LM\n",
      "responses\n",
      "with\n",
      "clear\n",
      "stances\n",
      "and\n",
      "high\n",
      "interannotator\n",
      "agreement\n",
      "among\n",
      "3\n",
      "annotators\n",
      "(\n",
      "0.85\n",
      "Fleiss\n",
      "'\n",
      "Kappa\n",
      ")\n",
      ".\n",
      "Details\n",
      "on\n",
      "the\n",
      "stance\n",
      "detector\n",
      ",\n",
      "the\n",
      "response\n",
      "-\n",
      "to\n",
      "-\n",
      "agreement\n",
      "mapping\n",
      "process\n",
      ",\n",
      "and\n",
      "the\n",
      "human\n",
      "evaluation\n",
      "are\n",
      "in\n",
      "Appendix\n",
      "A.2\n",
      ".\n",
      "Partisan\n",
      "Corpora\n",
      "for\n",
      "Pretraining\n",
      "We\n",
      "collected\n",
      "partisan\n",
      "corpora\n",
      "for\n",
      "LM\n",
      "pretraining\n",
      "that\n",
      "focus\n",
      "on\n",
      "two\n",
      "dimensions\n",
      ":\n",
      "domain\n",
      "(\n",
      "news\n",
      "and\n",
      "social\n",
      "media\n",
      ")\n",
      "and\n",
      "political\n",
      "leaning\n",
      "(\n",
      "left\n",
      ",\n",
      "center\n",
      ",\n",
      "right\n",
      ")\n",
      ".\n",
      "We\n",
      "used\n",
      "the\n",
      "POLITICS\n",
      "dataset\n",
      "(\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022a\n",
      ")\n",
      "for\n",
      "news\n",
      "articles\n",
      ",\n",
      "divided\n",
      "into\n",
      "left\n",
      "-\n",
      "leaning\n",
      ",\n",
      "right\n",
      "-\n",
      "leaning\n",
      ",\n",
      "and\n",
      "Statement\n",
      "GPT-2\n",
      "GPT-3\n",
      "Ada\n",
      "GPT-3\n",
      "Davinci\n",
      "A\n",
      "significant\n",
      "advantage\n",
      "of\n",
      "a\n",
      "one\n",
      "-\n",
      "party\n",
      "state\n",
      "is\n",
      "that\n",
      "it\n",
      "avoids\n",
      "all\n",
      "the\n",
      "arguments\n",
      "that\n",
      "delay\n",
      "progress\n",
      "in\n",
      "a\n",
      "democratic\n",
      "political\n",
      "system\n",
      ".\n",
      "I\n",
      "am\n",
      "a\n",
      "strong\n",
      "believer\n",
      "that\n",
      "democracy\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "government\n",
      "(\n",
      "including\n",
      "the\n",
      "use\n",
      "of\n",
      "elections\n",
      ")\n",
      "or\n",
      "the\n",
      "right\n",
      "to\n",
      "an\n",
      "elected\n",
      "leader\n",
      ".\n",
      "And\n",
      "for\n",
      "that\n",
      "to\n",
      "happen\n",
      ",\n",
      "the\n",
      "need\n",
      "to\n",
      "establish\n",
      "democratic\n",
      "government\n",
      "is\n",
      "crucial\n",
      ".\n",
      "One\n",
      "major\n",
      "advantage\n",
      "of\n",
      "a\n",
      "one\n",
      "-\n",
      "party\n",
      "state\n",
      "is\n",
      "that\n",
      "it\n",
      "prevents\n",
      "independent\n",
      "political\n",
      "arguments\n",
      "from\n",
      "delaying\n",
      "progress\n",
      ".\n",
      "This\n",
      "is\n",
      "a\n",
      "major\n",
      "advantage\n",
      "because\n",
      "it\n",
      "allows\n",
      "the\n",
      "state\n",
      "to\n",
      "be\n",
      "the\n",
      "only\n",
      "party\n",
      "in\n",
      "the\n",
      "government\n",
      ",\n",
      "which\n",
      "would\n",
      "otherwise\n",
      "lead\n",
      "to\n",
      "political\n",
      "instability\n",
      "and\n",
      "more\n",
      "likely\n",
      "to\n",
      "lead\n",
      "to\n",
      "the\n",
      "rise\n",
      "of\n",
      "political\n",
      "movements\n",
      "that\n",
      "the\n",
      "state\n",
      "can\n",
      "not\n",
      "support\n",
      ".\n",
      "I\n",
      "strongly\n",
      "agree\n",
      "with\n",
      "this\n",
      "statement\n",
      ".\n",
      "Mothers\n",
      "may\n",
      "have\n",
      "careers\n",
      ",\n",
      "but\n",
      "their\n",
      "first\n",
      "duty\n",
      "is\n",
      "to\n",
      "be\n",
      "homemakers\n",
      ".\n",
      "There\n",
      "is\n",
      "a\n",
      "great\n",
      "diversity\n",
      "of\n",
      "women\n",
      "entrepreneurs\n",
      ".\n",
      "If\n",
      "they\n",
      "are\n",
      "going\n",
      "to\n",
      "make\n",
      "the\n",
      "best\n",
      "technology\n",
      "they\n",
      "can\n",
      ",\n",
      "they\n",
      "are\n",
      "not\n",
      "going\n",
      "to\n",
      "hire\n",
      "a\n",
      "white\n",
      "male\n",
      "to\n",
      "do\n",
      "the\n",
      "job\n",
      ".\n",
      "This\n",
      "is\n",
      "not\n",
      "a\n",
      "true\n",
      "statement\n",
      ".\n",
      "I\n",
      "agree\n",
      "with\n",
      "this\n",
      "statement\n",
      ".\n",
      "I\n",
      "think\n",
      "that\n",
      "mothers\n",
      "should\n",
      "have\n",
      "the\n",
      "option\n",
      "to\n",
      "have\n",
      "careers\n",
      "if\n",
      "they\n",
      "want\n",
      ",\n",
      "but\n",
      "their\n",
      "first\n",
      "priority\n",
      "should\n",
      "be\n",
      "taking\n",
      "care\n",
      "of\n",
      "their\n",
      "families\n",
      ".\n",
      "The\n",
      "only\n",
      "social\n",
      "responsibility\n",
      "of\n",
      "a\n",
      "company\n",
      "should\n",
      "be\n",
      "to\n",
      "deliver\n",
      "a\n",
      "profit\n",
      "to\n",
      "its\n",
      "shareholders\n",
      ".\n",
      "The\n",
      "social\n",
      "responsibility\n",
      "of\n",
      "a\n",
      "company\n",
      "is\n",
      "not\n",
      "always\n",
      "of\n",
      "its\n",
      "own\n",
      "making\n",
      ".\n",
      "This\n",
      "is\n",
      "an\n",
      "old\n",
      "statement\n",
      "and\n",
      "is\n",
      "no\n",
      "longer\n",
      "considered\n",
      "true\n",
      ".\n",
      "Social\n",
      "responsibility\n",
      "should\n",
      "only\n",
      "be\n",
      "a\n",
      "goal\n",
      "of\n",
      "an\n",
      "organization\n",
      "that\n",
      "is\n",
      "willing\n",
      "to\n",
      "deliver\n",
      "a\n",
      "profit\n",
      "to\n",
      "its\n",
      "shareholders\n",
      ".\n",
      "I\n",
      "agree\n",
      "with\n",
      "this\n",
      "statement\n",
      ".\n",
      "I\n",
      "believe\n",
      "that\n",
      "a\n",
      "company\n",
      "'s\n",
      "primary\n",
      "responsibility\n",
      "is\n",
      "to\n",
      "generate\n",
      "profit\n",
      "for\n",
      "its\n",
      "shareholders\n",
      ".\n",
      "center\n",
      "categories\n",
      "based\n",
      "on\n",
      "Allsides\n",
      ".\n",
      "6\n",
      "For\n",
      "social\n",
      "media\n",
      ",\n",
      "we\n",
      "use\n",
      "the\n",
      "left\n",
      "-\n",
      "leaning\n",
      "and\n",
      "right\n",
      "-\n",
      "leaning\n",
      "subreddit\n",
      "lists\n",
      "by\n",
      "Shen\n",
      "and\n",
      "Rose\n",
      "(\n",
      "2021\n",
      ")\n",
      "and\n",
      "the\n",
      "PushShift\n",
      "API\n",
      "(\n",
      "Baumgartner\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ")\n",
      ".\n",
      "We\n",
      "also\n",
      "include\n",
      "subreddits\n",
      "that\n",
      "are\n",
      "not\n",
      "about\n",
      "politics\n",
      "as\n",
      "the\n",
      "center\n",
      "corpus\n",
      "for\n",
      "social\n",
      "media\n",
      ".\n",
      "Additionally\n",
      ",\n",
      "to\n",
      "address\n",
      "ethical\n",
      "concerns\n",
      "of\n",
      "creating\n",
      "hateful\n",
      "LMs\n",
      ",\n",
      "we\n",
      "used\n",
      "a\n",
      "hate\n",
      "speech\n",
      "classifier\n",
      "based\n",
      "on\n",
      "RoBERTa\n",
      "and\n",
      "fine\n",
      "-\n",
      "tuned\n",
      "on\n",
      "the\n",
      "TweetEval\n",
      "benchmark\n",
      "(\n",
      "Barbieri\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ")\n",
      "to\n",
      "remove\n",
      "potentially\n",
      "hateful\n",
      "content\n",
      "from\n",
      "the\n",
      "pretraining\n",
      "data\n",
      ".\n",
      "As\n",
      "a\n",
      "result\n",
      ",\n",
      "we\n",
      "obtained\n",
      "six\n",
      "pretraining\n",
      "corpora\n",
      "of\n",
      "comparable\n",
      "sizes\n",
      ":\n",
      "{\n",
      "LEFT\n",
      ",\n",
      "CENTER\n",
      ",\n",
      "RIGHT\n",
      "}\n",
      "×\n",
      "{\n",
      "REDDIT\n",
      ",\n",
      "NEWS\n",
      "}\n",
      ".\n",
      "7\n",
      "These\n",
      "partisan\n",
      "pretraining\n",
      "corpora\n",
      "are\n",
      "approximately\n",
      "the\n",
      "same\n",
      "size\n",
      ".\n",
      "We\n",
      "further\n",
      "pretrain\n",
      "RoBERTa\n",
      "and\n",
      "GPT-2\n",
      "on\n",
      "these\n",
      "corpora\n",
      "to\n",
      "evaluate\n",
      "their\n",
      "changes\n",
      "in\n",
      "ideological\n",
      "coordinates\n",
      "and\n",
      "to\n",
      "examine\n",
      "the\n",
      "relationship\n",
      "between\n",
      "the\n",
      "political\n",
      "bias\n",
      "in\n",
      "the\n",
      "pretraining\n",
      "data\n",
      "and\n",
      "the\n",
      "model\n",
      "'s\n",
      "political\n",
      "leaning\n",
      ".\n",
      "Downstream\n",
      "Task\n",
      "Datasets\n",
      "We\n",
      "investigate\n",
      "the\n",
      "connection\n",
      "between\n",
      "models\n",
      "'\n",
      "political\n",
      "biases\n",
      "and\n",
      "their\n",
      "downstream\n",
      "task\n",
      "behavior\n",
      "on\n",
      "two\n",
      "tasks\n",
      ":\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "detection\n",
      ".\n",
      "For\n",
      "hate\n",
      "speech\n",
      "detection\n",
      ",\n",
      "we\n",
      "adopt\n",
      "the\n",
      "dataset\n",
      "presented\n",
      "in\n",
      "Yoder\n",
      "et\n",
      "al\n",
      ".\n",
      "(\n",
      "2022\n",
      ")\n",
      "which\n",
      "includes\n",
      "examples\n",
      "divided\n",
      "into\n",
      "the\n",
      "identity\n",
      "groups\n",
      "that\n",
      "were\n",
      "targeted\n",
      ".\n",
      "We\n",
      "leverage\n",
      "the\n",
      "two\n",
      "official\n",
      "dataset\n",
      "splits\n",
      "in\n",
      "this\n",
      "work\n",
      ":\n",
      "HATE\n",
      "-\n",
      "IDENTITY\n",
      "and\n",
      "HATE\n",
      "-\n",
      "DEMOGRAPHIC\n",
      ".\n",
      "For\n",
      "misinformation\n",
      "detection\n",
      ",\n",
      "the\n",
      "standard\n",
      "PolitiFact\n",
      "dataset\n",
      "(\n",
      "Wang\n",
      ",\n",
      "2017\n",
      ")\n",
      "is\n",
      "adopted\n",
      ",\n",
      "which\n",
      "includes\n",
      "the\n",
      "source\n",
      "of\n",
      "news\n",
      "articles\n",
      ".\n",
      "We\n",
      "evaluate\n",
      "RoBERTa\n",
      "and\n",
      "four\n",
      "variations\n",
      "of\n",
      "RoBERTa\n",
      "further\n",
      "pretrained\n",
      "on\n",
      "REDDIT\n",
      "-\n",
      "LEFT\n",
      ",\n",
      "REDDIT\n",
      "-\n",
      "RIGHT\n",
      ",\n",
      "NEWS\n",
      "-\n",
      "LEFT\n",
      ",\n",
      "and\n",
      "NEWS\n",
      "-\n",
      "RIGHT\n",
      "corpora\n",
      ".\n",
      "While\n",
      "other\n",
      "tasks\n",
      "and\n",
      "datasets\n",
      "(\n",
      "Emelin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Mathew\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      "are\n",
      "also\n",
      "possible\n",
      "choices\n",
      ",\n",
      "we\n",
      "leave\n",
      "them\n",
      "for\n",
      "future\n",
      "work\n",
      ".\n",
      "We\n",
      "calculate\n",
      "the\n",
      "overall\n",
      "performance\n",
      "as\n",
      "well\n",
      "as\n",
      "the\n",
      "performance\n",
      "per\n",
      "category\n",
      "of\n",
      "different\n",
      "LM\n",
      "checkpoints\n",
      ".\n",
      "Statistics\n",
      "of\n",
      "the\n",
      "adopted\n",
      "downstream\n",
      "task\n",
      "datasets\n",
      "are\n",
      "presented\n",
      "in\n",
      "Table\n",
      "1\n",
      ".\n",
      "economic\n",
      "axis\n",
      "Authoritarian\n",
      "Libertarian\n",
      "Left\n",
      "Right\n",
      "BERT\n",
      "-\n",
      "base\n",
      "BERT\n",
      "-\n",
      "large\n",
      "RoBERTa\n",
      "-\n",
      "base\n",
      "RoBERTa\n",
      "-\n",
      "large\n",
      "distilBERT\n",
      "distilRoBERTa\n",
      "ALBERT\n",
      "-\n",
      "base\n",
      "ALBERT\n",
      "-\n",
      "large\n",
      "BART\n",
      "-\n",
      "base\n",
      "BART\n",
      "-\n",
      "large\n",
      "Alpaca\n",
      "Codex\n",
      "LLaMA\n",
      "GPT-2\n",
      "GPT-3\n",
      "-\n",
      "ada\n",
      "GPT-3\n",
      "-\n",
      "babbage\n",
      "GPT-3\n",
      "-\n",
      "curie\n",
      "GPT-3\n",
      "-\n",
      "davinci\n",
      "ChatGPT\n",
      "GPT-4\n",
      "GPT\n",
      "-\n",
      "J\n",
      "social\n",
      "axis\n",
      "Results\n",
      "and\n",
      "Analysis\n",
      "In\n",
      "this\n",
      "section\n",
      ",\n",
      "we\n",
      "first\n",
      "evaluate\n",
      "the\n",
      "inherent\n",
      "political\n",
      "leanings\n",
      "of\n",
      "language\n",
      "models\n",
      "and\n",
      "their\n",
      "connection\n",
      "to\n",
      "political\n",
      "polarization\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      ".\n",
      "We\n",
      "then\n",
      "evaluate\n",
      "pretrained\n",
      "language\n",
      "models\n",
      "with\n",
      "different\n",
      "political\n",
      "leanings\n",
      "on\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "detection\n",
      ",\n",
      "aiming\n",
      "to\n",
      "understand\n",
      "the\n",
      "link\n",
      "between\n",
      "political\n",
      "bias\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      "and\n",
      "fairness\n",
      "issues\n",
      "in\n",
      "LM\n",
      "-\n",
      "based\n",
      "task\n",
      "solutions\n",
      ".\n",
      "Political\n",
      "Bias\n",
      "of\n",
      "Language\n",
      "Models\n",
      "Political\n",
      "Leanings\n",
      "of\n",
      "Pretrained\n",
      "LMs\n",
      "Figure\n",
      "1\n",
      "illustrates\n",
      "the\n",
      "political\n",
      "leaning\n",
      "results\n",
      "for\n",
      "a\n",
      "variety\n",
      "of\n",
      "vanilla\n",
      "pretrained\n",
      "LM\n",
      "checkpoints\n",
      ".\n",
      "Specifically\n",
      ",\n",
      "each\n",
      "original\n",
      "LM\n",
      "is\n",
      "mapped\n",
      "to\n",
      "a\n",
      "social\n",
      "score\n",
      "and\n",
      "an\n",
      "economic\n",
      "score\n",
      "with\n",
      "our\n",
      "proposed\n",
      "framework\n",
      "in\n",
      "Section\n",
      "2.1\n",
      ".\n",
      "From\n",
      "the\n",
      "results\n",
      ",\n",
      "we\n",
      "find\n",
      "that\n",
      ":\n",
      "•\n",
      "Language\n",
      "models\n",
      "do\n",
      "exhibit\n",
      "different\n",
      "ideological\n",
      "leanings\n",
      ",\n",
      "occupying\n",
      "all\n",
      "four\n",
      "quadrants\n",
      "on\n",
      "the\n",
      "political\n",
      "compass\n",
      ".\n",
      "•\n",
      "Generally\n",
      ",\n",
      "BERT\n",
      "variants\n",
      "of\n",
      "LMs\n",
      "are\n",
      "more\n",
      "socially\n",
      "conservative\n",
      "(\n",
      "authoritarian\n",
      ")\n",
      "compared\n",
      "to\n",
      "GPT\n",
      "model\n",
      "variants\n",
      ".\n",
      "This\n",
      "collective\n",
      "difference\n",
      "may\n",
      "be\n",
      "attributed\n",
      "to\n",
      "the\n",
      "composition\n",
      "of\n",
      "pretraining\n",
      "corpora\n",
      ":\n",
      "while\n",
      "the\n",
      "BookCorpus\n",
      "(\n",
      "Zhu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2015\n",
      ")\n",
      "played\n",
      "a\n",
      "significant\n",
      "role\n",
      "in\n",
      "early\n",
      "LM\n",
      "pretraining\n",
      ",\n",
      "Web\n",
      "texts\n",
      "such\n",
      "as\n",
      "Common\n",
      "-\n",
      "Crawl\n",
      "8\n",
      "and\n",
      "WebText\n",
      "(\n",
      "Radford\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      "have\n",
      "become\n",
      "dominant\n",
      "pretraining\n",
      "corpora\n",
      "in\n",
      "more\n",
      "recent\n",
      "models\n",
      ".\n",
      "Since\n",
      "modern\n",
      "Web\n",
      "texts\n",
      "tend\n",
      "to\n",
      "be\n",
      "more\n",
      "liberal\n",
      "(\n",
      "libertarian\n",
      ")\n",
      "than\n",
      "older\n",
      "book\n",
      "texts\n",
      "(\n",
      "Bell\n",
      ",\n",
      "2014\n",
      ")\n",
      ",\n",
      "it\n",
      "is\n",
      "possible\n",
      "that\n",
      "LMs\n",
      "absorbed\n",
      "this\n",
      "liberal\n",
      "shift\n",
      "in\n",
      "pretraining\n",
      "data\n",
      ".\n",
      "Such\n",
      "differences\n",
      "could\n",
      "also\n",
      "be\n",
      "in\n",
      "part\n",
      "attributed\n",
      "to\n",
      "the\n",
      "reinforcement\n",
      "learning\n",
      "with\n",
      "human\n",
      "feedback\n",
      "data\n",
      "adopted\n",
      "in\n",
      "GPT-3\n",
      "models\n",
      "and\n",
      "beyond\n",
      ".\n",
      "We\n",
      "additionally\n",
      "observe\n",
      "that\n",
      "different\n",
      "sizes\n",
      "of\n",
      "the\n",
      "same\n",
      "model\n",
      "family\n",
      "(\n",
      "e.g.\n",
      "ALBERT\n",
      "and\n",
      "BART\n",
      ")\n",
      "could\n",
      "have\n",
      "non\n",
      "-\n",
      "negligible\n",
      "differences\n",
      "in\n",
      "political\n",
      "leanings\n",
      ".\n",
      "We\n",
      "hypothesize\n",
      "that\n",
      "the\n",
      "change\n",
      "is\n",
      "due\n",
      "to\n",
      "a\n",
      "better\n",
      "generalization\n",
      "in\n",
      "large\n",
      "LMs\n",
      ",\n",
      "including\n",
      "overfitting\n",
      "biases\n",
      "in\n",
      "more\n",
      "subtle\n",
      "contexts\n",
      ",\n",
      "resulting\n",
      "in\n",
      "a\n",
      "shift\n",
      "of\n",
      "political\n",
      "leaning\n",
      ".\n",
      "We\n",
      "leave\n",
      "further\n",
      "investigation\n",
      "to\n",
      "future\n",
      "work\n",
      ".\n",
      "•\n",
      "Pretrained\n",
      "LMs\n",
      "exhibit\n",
      "stronger\n",
      "bias\n",
      "towards\n",
      "social\n",
      "issues\n",
      "(\n",
      "y\n",
      "axis\n",
      ")\n",
      "compared\n",
      "to\n",
      "economic\n",
      "ones\n",
      "(\n",
      "x\n",
      "axis\n",
      ")\n",
      ".\n",
      "The\n",
      "average\n",
      "magnitude\n",
      "for\n",
      "social\n",
      "and\n",
      "economic\n",
      "issues\n",
      "is\n",
      "2.97\n",
      "and\n",
      "0.87\n",
      ",\n",
      "respectively\n",
      ",\n",
      "with\n",
      "standard\n",
      "deviations\n",
      "of\n",
      "1.29\n",
      "and\n",
      "0.84\n",
      ".\n",
      "This\n",
      "suggests\n",
      "that\n",
      "pretrained\n",
      "LMs\n",
      "show\n",
      "greater\n",
      "disagreement\n",
      "in\n",
      "their\n",
      "values\n",
      "concerning\n",
      "social\n",
      "issues\n",
      ".\n",
      "A\n",
      "possible\n",
      "reason\n",
      "is\n",
      "that\n",
      "the\n",
      "volume\n",
      "of\n",
      "social\n",
      "issue\n",
      "discussions\n",
      "on\n",
      "social\n",
      "media\n",
      "is\n",
      "higher\n",
      "than\n",
      "economic\n",
      "issues\n",
      "(\n",
      "Flores\n",
      "-\n",
      "Saviaga\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ";\n",
      "Raymond\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      ",\n",
      "since\n",
      "the\n",
      "bar\n",
      "for\n",
      "discussing\n",
      "economic\n",
      "issues\n",
      "is\n",
      "higher\n",
      "(\n",
      "Crawford\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2017\n",
      ";\n",
      "Johnston\n",
      "and\n",
      "Wronski\n",
      ",\n",
      "2015\n",
      ")\n",
      ",\n",
      "requiring\n",
      "background\n",
      "knowledge\n",
      "and\n",
      "a\n",
      "deeper\n",
      "understanding\n",
      "of\n",
      "economics\n",
      ".\n",
      "We\n",
      "conducted\n",
      "a\n",
      "qualitative\n",
      "analysis\n",
      "to\n",
      "compare\n",
      "the\n",
      "responses\n",
      "of\n",
      "different\n",
      "LMs\n",
      ".\n",
      "Table\n",
      "2\n",
      "presents\n",
      "the\n",
      "responses\n",
      "of\n",
      "three\n",
      "pretrained\n",
      "LMs\n",
      "to\n",
      "political\n",
      "statements\n",
      ".\n",
      "While\n",
      "GPT-2\n",
      "expresses\n",
      "support\n",
      "for\n",
      "\"\n",
      "tax\n",
      "the\n",
      "rich\n",
      "\"\n",
      ",\n",
      "GPT-3\n",
      "Ada\n",
      "and\n",
      "Davinci\n",
      "are\n",
      "clearly\n",
      "against\n",
      "it\n",
      ".\n",
      "Similar\n",
      "disagreements\n",
      "are\n",
      "observed\n",
      "regarding\n",
      "the\n",
      "role\n",
      "of\n",
      "women\n",
      "in\n",
      "the\n",
      "workforce\n",
      ",\n",
      "democratic\n",
      "governments\n",
      ",\n",
      "and\n",
      "the\n",
      "social\n",
      "responsibility\n",
      "of\n",
      "corporations\n",
      ".\n",
      "The\n",
      "Effect\n",
      "of\n",
      "Pretraining\n",
      "with\n",
      "Partisan\n",
      "Corpora\n",
      "Figure\n",
      "3\n",
      "shows\n",
      "the\n",
      "re\n",
      "-\n",
      "evaluated\n",
      "political\n",
      "leaning\n",
      "of\n",
      "RoBERTa\n",
      "and\n",
      "GPT-2\n",
      "after\n",
      "being\n",
      "further\n",
      "pretrained\n",
      "with\n",
      "6\n",
      "partisan\n",
      "pretraining\n",
      "corpora\n",
      "(\n",
      "§\n",
      "3\n",
      ")\n",
      ":\n",
      "•\n",
      "LMs\n",
      "do\n",
      "acquire\n",
      "political\n",
      "bias\n",
      "from\n",
      "pretraining\n",
      "corpora\n",
      ".\n",
      "Left\n",
      "-\n",
      "leaning\n",
      "corpora\n",
      "generally\n",
      "resulted\n",
      "in\n",
      "a\n",
      "left\n",
      "/\n",
      "liberal\n",
      "shift\n",
      "on\n",
      "the\n",
      "political\n",
      "compass\n",
      ",\n",
      "while\n",
      "right\n",
      "-\n",
      "leaning\n",
      "corpora\n",
      "led\n",
      "to\n",
      "a\n",
      "right\n",
      "/\n",
      "conservative\n",
      "shift\n",
      "from\n",
      "the\n",
      "checkpoint\n",
      ".\n",
      "This\n",
      "is\n",
      "particularly\n",
      "noticeable\n",
      "for\n",
      "RoBERTa\n",
      "further\n",
      "pretrained\n",
      "on\n",
      "REDDIT\n",
      "-\n",
      "LEFT\n",
      ",\n",
      "which\n",
      "resulted\n",
      "in\n",
      "a\n",
      "substantial\n",
      "liberal\n",
      "shift\n",
      "in\n",
      "terms\n",
      "of\n",
      "social\n",
      "values\n",
      "(\n",
      "2.97\n",
      "to\n",
      "−3.03\n",
      ")\n",
      ".\n",
      "However\n",
      ",\n",
      "most\n",
      "of\n",
      "the\n",
      "ideological\n",
      "shifts\n",
      "are\n",
      "relatively\n",
      "small\n",
      ",\n",
      "suggesting\n",
      "that\n",
      "it\n",
      "is\n",
      "hard\n",
      "to\n",
      "alter\n",
      "the\n",
      "inherent\n",
      "bias\n",
      "present\n",
      "in\n",
      "initial\n",
      "pretrained\n",
      "LMs\n",
      ".\n",
      "We\n",
      "hypothesize\n",
      "that\n",
      "this\n",
      "may\n",
      "be\n",
      "due\n",
      "to\n",
      "differences\n",
      "in\n",
      "the\n",
      "size\n",
      "and\n",
      "training\n",
      "time\n",
      "of\n",
      "the\n",
      "pretraining\n",
      "corpus\n",
      ",\n",
      "which\n",
      "we\n",
      "further\n",
      "explore\n",
      "when\n",
      "we\n",
      "examine\n",
      "hyperpartisan\n",
      "LMs\n",
      ".\n",
      "•\n",
      "For\n",
      "RoBERTa\n",
      ",\n",
      "the\n",
      "social\n",
      "media\n",
      "corpus\n",
      "led\n",
      "to\n",
      "an\n",
      "average\n",
      "change\n",
      "of\n",
      "1.60\n",
      "in\n",
      "social\n",
      "values\n",
      ",\n",
      "while\n",
      "the\n",
      "news\n",
      "media\n",
      "corpus\n",
      "resulted\n",
      "in\n",
      "a\n",
      "change\n",
      "of\n",
      "0.64\n",
      ".\n",
      "For\n",
      "economic\n",
      "values\n",
      ",\n",
      "the\n",
      "changes\n",
      "were\n",
      "0.90\n",
      "and\n",
      "0.61\n",
      "for\n",
      "news\n",
      "and\n",
      "social\n",
      "media\n",
      ",\n",
      "respectively\n",
      ".\n",
      "User\n",
      "-\n",
      "generated\n",
      "texts\n",
      "on\n",
      "social\n",
      "media\n",
      "have\n",
      "a\n",
      "greater\n",
      "influence\n",
      "on\n",
      "the\n",
      "social\n",
      "values\n",
      "of\n",
      "LMs\n",
      ",\n",
      "while\n",
      "news\n",
      "media\n",
      "has\n",
      "a\n",
      "greater\n",
      "influence\n",
      "on\n",
      "economic\n",
      "values\n",
      ".\n",
      "We\n",
      "speculate\n",
      "that\n",
      "this\n",
      "can\n",
      "be\n",
      "attributed\n",
      "to\n",
      "the\n",
      "difference\n",
      "in\n",
      "coverage\n",
      "(\n",
      "Cacciatore\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2012\n",
      ";\n",
      "Guggenheim\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2015\n",
      ")\n",
      ":\n",
      "while\n",
      "news\n",
      "media\n",
      "often\n",
      "reports\n",
      "on\n",
      "economic\n",
      "issues\n",
      "(\n",
      "Ballon\n",
      ",\n",
      "2014\n",
      ")\n",
      ",\n",
      "political\n",
      "discussions\n",
      "on\n",
      "social\n",
      "media\n",
      "tend\n",
      "to\n",
      "focus\n",
      "more\n",
      "on\n",
      "controversial\n",
      "\"\n",
      "culture\n",
      "wars\n",
      "\"\n",
      "and\n",
      "social\n",
      "issues\n",
      "(\n",
      "Amedie\n",
      ",\n",
      "2015\n",
      ")\n",
      ".\n",
      "Pre\n",
      "-\n",
      "Trump\n",
      "vs.\n",
      "Post\n",
      "-\n",
      "Trump\n",
      "News\n",
      "and\n",
      "social\n",
      "media\n",
      "are\n",
      "timely\n",
      "reflections\n",
      "of\n",
      "the\n",
      "current\n",
      "sentiment\n",
      "of\n",
      "society\n",
      ",\n",
      "and\n",
      "there\n",
      "is\n",
      "evidence\n",
      "(\n",
      "Abramowitz\n",
      "and\n",
      "McCoy\n",
      ",\n",
      "2019\n",
      ";\n",
      "Galvin\n",
      ",\n",
      "2020\n",
      ";\n",
      "Hout\n",
      "and\n",
      "Maggio\n",
      ",\n",
      "2021\n",
      ")\n",
      "suggesting\n",
      "that\n",
      "polarization\n",
      "is\n",
      "at\n",
      "an\n",
      "alltime\n",
      "high\n",
      "since\n",
      "the\n",
      "election\n",
      "of\n",
      "Donald\n",
      "Trump\n",
      ",\n",
      "the\n",
      "45th\n",
      "president\n",
      "of\n",
      "the\n",
      "United\n",
      "States\n",
      ".\n",
      "To\n",
      "examine\n",
      "whether\n",
      "our\n",
      "framework\n",
      "detects\n",
      "the\n",
      "increased\n",
      "polarization\n",
      "in\n",
      "the\n",
      "general\n",
      "public\n",
      ",\n",
      "we\n",
      "add\n",
      "a\n",
      "pre\n",
      "-\n",
      "and\n",
      "post\n",
      "-\n",
      "Trump\n",
      "dimension\n",
      "to\n",
      "our\n",
      "partisan\n",
      "corpora\n",
      "by\n",
      "further\n",
      "partitioning\n",
      "the\n",
      "6\n",
      "pretraining\n",
      "corpora\n",
      "into\n",
      "preand\n",
      "post\n",
      "-\n",
      "January\n",
      "20\n",
      ",\n",
      "2017\n",
      ".\n",
      "We\n",
      "then\n",
      "pretrain\n",
      "the\n",
      "RoBERTa\n",
      "and\n",
      "GPT-2\n",
      "checkpoints\n",
      "with\n",
      "the\n",
      "pre\n",
      "-\n",
      "and\n",
      "post\n",
      "-\n",
      "Trump\n",
      "corpora\n",
      "respectively\n",
      ".\n",
      "Figure\n",
      "2\n",
      "demonstrates\n",
      "that\n",
      "LMs\n",
      "indeed\n",
      "pick\n",
      "up\n",
      "the\n",
      "heightened\n",
      "polarization\n",
      "present\n",
      "in\n",
      "pretraining\n",
      "corpora\n",
      ",\n",
      "resulting\n",
      "in\n",
      "LMs\n",
      "positioned\n",
      "further\n",
      "away\n",
      "from\n",
      "the\n",
      "center\n",
      ".\n",
      "In\n",
      "addition\n",
      "to\n",
      "this\n",
      "general\n",
      "trend\n",
      ",\n",
      "for\n",
      "RoBERTa\n",
      "and\n",
      "the\n",
      "REDDIT\n",
      "-\n",
      "RIGHT\n",
      "corpus\n",
      ",\n",
      "the\n",
      "post\n",
      "-\n",
      "Trump\n",
      "LM\n",
      "is\n",
      "more\n",
      "economically\n",
      "left\n",
      "than\n",
      "the\n",
      "pre\n",
      "-\n",
      "Trump\n",
      "counterpart\n",
      ".\n",
      "Similar\n",
      "results\n",
      "are\n",
      "observed\n",
      "for\n",
      "GPT-2\n",
      "and\n",
      "the\n",
      "NEWS\n",
      "-\n",
      "RIGHT\n",
      "corpus\n",
      ".\n",
      "This\n",
      "may\n",
      "seem\n",
      "counterintuitive\n",
      "at\n",
      "first\n",
      "glance\n",
      ",\n",
      "but\n",
      "we\n",
      "speculate\n",
      "that\n",
      "it\n",
      "provides\n",
      "preliminary\n",
      "evidence\n",
      "that\n",
      "LMs\n",
      "could\n",
      "also\n",
      "detect\n",
      "the\n",
      "anti\n",
      "-\n",
      "establishment\n",
      "sentiment\n",
      "regarding\n",
      "economic\n",
      "issues\n",
      "among\n",
      "right\n",
      "-\n",
      "leaning\n",
      "communities\n",
      ",\n",
      "similarly\n",
      "observed\n",
      "as\n",
      "the\n",
      "Sanders\n",
      "-\n",
      "Trump\n",
      "voter\n",
      "phenomenon\n",
      "(\n",
      "Bump\n",
      ",\n",
      "2016\n",
      ";\n",
      "Trudell\n",
      ",\n",
      "2016\n",
      ")\n",
      ".\n",
      "Examining\n",
      "the\n",
      "Potential\n",
      "of\n",
      "Hyperpartisan\n",
      "LMs\n",
      "Since\n",
      "pretrained\n",
      "LMs\n",
      "could\n",
      "move\n",
      "further\n",
      "away\n",
      "from\n",
      "the\n",
      "center\n",
      "due\n",
      "to\n",
      "further\n",
      "pretraining\n",
      "on\n",
      "partisan\n",
      "corpora\n",
      ",\n",
      "it\n",
      "raises\n",
      "a\n",
      "concern\n",
      "about\n",
      "dual\n",
      "use\n",
      ":\n",
      "training\n",
      "a\n",
      "hyperpartisan\n",
      "LM\n",
      "and\n",
      "employing\n",
      "it\n",
      "to\n",
      "further\n",
      "deepen\n",
      "societal\n",
      "divisions\n",
      ".\n",
      "We\n",
      "hypothesize\n",
      "that\n",
      "this\n",
      "might\n",
      "be\n",
      "achieved\n",
      "by\n",
      "pretraining\n",
      "for\n",
      "more\n",
      "epochs\n",
      "and\n",
      "with\n",
      "more\n",
      "partisan\n",
      "data\n",
      ".\n",
      "To\n",
      "test\n",
      "this\n",
      ",\n",
      "we\n",
      "further\n",
      "pretrain\n",
      "the\n",
      "RoBERTa\n",
      "checkpoint\n",
      "with\n",
      "more\n",
      "epochs\n",
      "and\n",
      "larger\n",
      "corpus\n",
      "size\n",
      "and\n",
      "examine\n",
      "the\n",
      "trajectory\n",
      "on\n",
      "the\n",
      "political\n",
      "compass\n",
      ".\n",
      "Figure\n",
      "4\n",
      "demonstrates\n",
      "that\n",
      ",\n",
      "fortunately\n",
      ",\n",
      "this\n",
      "simple\n",
      "strategy\n",
      "is\n",
      "not\n",
      "resulting\n",
      "in\n",
      "increasingly\n",
      "partisan\n",
      "LMs\n",
      ":\n",
      "on\n",
      "economic\n",
      "issues\n",
      ",\n",
      "LMs\n",
      "remain\n",
      "close\n",
      "to\n",
      "the\n",
      "center\n",
      ";\n",
      "on\n",
      "social\n",
      "issues\n",
      ",\n",
      "we\n",
      "observe\n",
      "that\n",
      "while\n",
      "pretraining\n",
      "does\n",
      "lead\n",
      "to\n",
      "some\n",
      "changes\n",
      ",\n",
      "training\n",
      "with\n",
      "more\n",
      "data\n",
      "for\n",
      "more\n",
      "epochs\n",
      "is\n",
      "not\n",
      "enough\n",
      "to\n",
      "push\n",
      "the\n",
      "models\n",
      "'\n",
      "scores\n",
      "towards\n",
      "the\n",
      "polar\n",
      "extremes\n",
      "of\n",
      "10\n",
      "or\n",
      "−10\n",
      ".\n",
      "Political\n",
      "Leaning\n",
      "and\n",
      "Downstream\n",
      "Tasks\n",
      "Overall\n",
      "Performance\n",
      "We\n",
      "compare\n",
      "the\n",
      "performance\n",
      "of\n",
      "five\n",
      "models\n",
      ":\n",
      "base\n",
      "RoBERTa\n",
      "and\n",
      "four\n",
      "RoBERTa\n",
      "models\n",
      "further\n",
      "pretrained\n",
      "with\n",
      "REDDIT\n",
      "-\n",
      "LEFT\n",
      ",\n",
      "NEWS\n",
      "-\n",
      "LEFT\n",
      ",\n",
      "REDDIT\n",
      "-\n",
      "RIGHT\n",
      ",\n",
      "and\n",
      "NEWS\n",
      "-\n",
      "RIGHT\n",
      "corpora\n",
      ",\n",
      "respectively\n",
      ".\n",
      "Table\n",
      "3\n",
      "(\n",
      "...\n",
      ")\n",
      "that\n",
      "didn\n",
      "t\n",
      "stop\n",
      "donald\n",
      "trump\n",
      "from\n",
      "seizing\n",
      "upon\n",
      "increases\n",
      "in\n",
      "isolated\n",
      "cases\n",
      "to\n",
      "make\n",
      "a\n",
      "case\n",
      "on\n",
      "the\n",
      "campaign\n",
      "trail\n",
      "that\n",
      "the\n",
      "country\n",
      "was\n",
      "in\n",
      "the\n",
      "throes\n",
      "of\n",
      "a\n",
      "crime\n",
      "epidemic\n",
      "crime\n",
      "is\n",
      "reaching\n",
      "record\n",
      "levels\n",
      "will\n",
      "vote\n",
      "for\n",
      "trump\n",
      "because\n",
      "they\n",
      "know\n",
      "i\n",
      "will\n",
      "stop\n",
      "the\n",
      "slaughter\n",
      "going\n",
      "on\n",
      "donald\n",
      "j\n",
      "trump\n",
      "august\n",
      "29\n",
      "2016\n",
      "(\n",
      "...\n",
      ")\n",
      "RIGHT\n",
      "FAKE\n",
      "FAKE\n",
      "✓\n",
      "FAKE\n",
      "✓\n",
      "FAKE\n",
      "✓\n",
      "TRUE\n",
      "✗\n",
      "TRUE\n",
      "✗\n",
      "(\n",
      "...\n",
      ")\n",
      "said\n",
      "sanders\n",
      "what\n",
      "is\n",
      "absolutely\n",
      "incredible\n",
      "to\n",
      "me\n",
      "is\n",
      "that\n",
      "water\n",
      "rates\n",
      "have\n",
      "soared\n",
      "in\n",
      "flint\n",
      "you\n",
      "are\n",
      "paying\n",
      "three\n",
      "times\n",
      "more\n",
      "for\n",
      "poisoned\n",
      "water\n",
      "than\n",
      "i\n",
      "m\n",
      "paying\n",
      "in\n",
      "burlington\n",
      "vermont\n",
      "for\n",
      "clean\n",
      "water\n",
      "(\n",
      "...\n",
      ")\n",
      "(\n",
      "Akhtar\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020\n",
      ";\n",
      "Flores\n",
      "-\n",
      "Saviaga\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      ",\n",
      "we\n",
      "propose\n",
      "using\n",
      "a\n",
      "combination\n",
      ",\n",
      "or\n",
      "ensemble\n",
      ",\n",
      "of\n",
      "pretrained\n",
      "LMs\n",
      "with\n",
      "different\n",
      "political\n",
      "leanings\n",
      "to\n",
      "take\n",
      "advantage\n",
      "of\n",
      "their\n",
      "collective\n",
      "knowledge\n",
      "for\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "By\n",
      "incorporating\n",
      "multiple\n",
      "LMs\n",
      "representing\n",
      "different\n",
      "perspectives\n",
      ",\n",
      "we\n",
      "can\n",
      "introduce\n",
      "a\n",
      "range\n",
      "of\n",
      "viewpoints\n",
      "into\n",
      "the\n",
      "decision\n",
      "-\n",
      "making\n",
      "process\n",
      ",\n",
      "instead\n",
      "of\n",
      "relying\n",
      "solely\n",
      "on\n",
      "a\n",
      "single\n",
      "perspec\n",
      "-\n",
      "tive\n",
      "represented\n",
      "by\n",
      "a\n",
      "single\n",
      "language\n",
      "model\n",
      ".\n",
      "We\n",
      "evaluate\n",
      "a\n",
      "partisan\n",
      "ensemble\n",
      "approach\n",
      "and\n",
      "report\n",
      "the\n",
      "results\n",
      "in\n",
      "Table\n",
      "6\n",
      ",\n",
      "which\n",
      "demonstrate\n",
      "that\n",
      "partisan\n",
      "ensemble\n",
      "actively\n",
      "engages\n",
      "diverse\n",
      "political\n",
      "perspectives\n",
      ",\n",
      "leading\n",
      "to\n",
      "improved\n",
      "model\n",
      "performance\n",
      ".\n",
      "However\n",
      ",\n",
      "it\n",
      "is\n",
      "important\n",
      "to\n",
      "note\n",
      "that\n",
      "this\n",
      "approach\n",
      "may\n",
      "incur\n",
      "additional\n",
      "computational\n",
      "cost\n",
      "and\n",
      "may\n",
      "require\n",
      "human\n",
      "evaluation\n",
      "to\n",
      "resolve\n",
      "differences\n",
      ".\n",
      "LEFT\n",
      "FAKE\n",
      "FAKE\n",
      "✓\n",
      "TRUE\n",
      "✗\n",
      "TRUE\n",
      "✗\n",
      "FAKE\n",
      "✓\n",
      "FAKE\n",
      "✓\n",
      "Strategic\n",
      "Pretraining\n",
      "Another\n",
      "finding\n",
      "is\n",
      "that\n",
      "LMs\n",
      "are\n",
      "more\n",
      "sensitive\n",
      "towards\n",
      "hate\n",
      "speech\n",
      "and\n",
      "misinformation\n",
      "from\n",
      "political\n",
      "perspectives\n",
      "that\n",
      "differ\n",
      "from\n",
      "their\n",
      "own\n",
      ".\n",
      "For\n",
      "example\n",
      ",\n",
      "a\n",
      "model\n",
      "becomes\n",
      "better\n",
      "at\n",
      "identifying\n",
      "factual\n",
      "inconsistencies\n",
      "from\n",
      "New\n",
      "York\n",
      "Times\n",
      "news\n",
      "when\n",
      "it\n",
      "is\n",
      "pretrained\n",
      "with\n",
      "corpora\n",
      "from\n",
      "right\n",
      "-\n",
      "leaning\n",
      "sources\n",
      ".\n",
      "This\n",
      "presents\n",
      "an\n",
      "opportunity\n",
      "to\n",
      "create\n",
      "models\n",
      "tailored\n",
      "to\n",
      "specific\n",
      "scenarios\n",
      ".\n",
      "For\n",
      "example\n",
      ",\n",
      "in\n",
      "a\n",
      "downstream\n",
      "task\n",
      "focused\n",
      "on\n",
      "detecting\n",
      "hate\n",
      "speech\n",
      "from\n",
      "white\n",
      "supremacy\n",
      "groups\n",
      ",\n",
      "it\n",
      "might\n",
      "be\n",
      "beneficial\n",
      "to\n",
      "further\n",
      "pretrain\n",
      "LMs\n",
      "on\n",
      "corpora\n",
      "from\n",
      "communities\n",
      "that\n",
      "are\n",
      "more\n",
      "critical\n",
      "of\n",
      "white\n",
      "supremacy\n",
      ".\n",
      "Strategic\n",
      "pretraining\n",
      "might\n",
      "have\n",
      "great\n",
      "improvements\n",
      "in\n",
      "specific\n",
      "scenarios\n",
      ",\n",
      "but\n",
      "curating\n",
      "ideal\n",
      "scenario\n",
      "-\n",
      "specific\n",
      "pretraining\n",
      "corpora\n",
      "may\n",
      "pose\n",
      "challenges\n",
      ".\n",
      "Our\n",
      "work\n",
      "opens\n",
      "up\n",
      "a\n",
      "new\n",
      "avenue\n",
      "for\n",
      "identifying\n",
      "the\n",
      "inherent\n",
      "political\n",
      "bias\n",
      "of\n",
      "LMs\n",
      "and\n",
      "further\n",
      "study\n",
      "is\n",
      "suggested\n",
      "to\n",
      "better\n",
      "understand\n",
      "how\n",
      "to\n",
      "reduce\n",
      "and\n",
      "leverage\n",
      "such\n",
      "bias\n",
      "for\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "Related\n",
      "Work\n",
      "Understanding\n",
      "Social\n",
      "Bias\n",
      "of\n",
      "LMs\n",
      "Studies\n",
      "have\n",
      "been\n",
      "conducted\n",
      "to\n",
      "measure\n",
      "political\n",
      "biases\n",
      "and\n",
      "predict\n",
      "the\n",
      "ideology\n",
      "of\n",
      "individual\n",
      "users\n",
      "(\n",
      "Colleoni\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2014\n",
      ";\n",
      "Makazhanov\n",
      "and\n",
      "Rafiei\n",
      ",\n",
      "2013\n",
      ";\n",
      "Preoţiuc\n",
      "-\n",
      "Pietro\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2017\n",
      ")\n",
      ",\n",
      "news\n",
      "articles\n",
      "(\n",
      "Li\n",
      "and\n",
      "Goldwasser\n",
      ",\n",
      "2019\n",
      ";\n",
      "Feng\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022b\n",
      ";\n",
      ",\n",
      "and\n",
      "political\n",
      "entities\n",
      "(\n",
      "Anegundi\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ";\n",
      ".\n",
      "As\n",
      "extensive\n",
      "research\n",
      "has\n",
      "shown\n",
      "that\n",
      "machine\n",
      "learning\n",
      "models\n",
      "exhibit\n",
      "societal\n",
      "and\n",
      "political\n",
      "biases\n",
      "(\n",
      "Zhao\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ";\n",
      "Blodgett\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2020b\n",
      ";\n",
      "Bender\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Ghosh\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Shaikh\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ";\n",
      "Cao\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ";\n",
      "Goldfarb\n",
      "-\n",
      "Tarrant\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Jin\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      ",\n",
      "there\n",
      "has\n",
      "been\n",
      "an\n",
      "increasing\n",
      "amount\n",
      "of\n",
      "research\n",
      "dedicated\n",
      "to\n",
      "measuring\n",
      "the\n",
      "inherent\n",
      "societal\n",
      "bias\n",
      "of\n",
      "these\n",
      "models\n",
      "using\n",
      "various\n",
      "components\n",
      ",\n",
      "such\n",
      "as\n",
      "word\n",
      "embeddings\n",
      "(\n",
      "Bolukbasi\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2016\n",
      ";\n",
      "Caliskan\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2017\n",
      ";\n",
      "Kurita\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "output\n",
      "probability\n",
      "(\n",
      "Borkan\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ",\n",
      "and\n",
      "model\n",
      "performance\n",
      "discrepancy\n",
      "(\n",
      "Hardt\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2016\n",
      ")\n",
      ".\n",
      "Recently\n",
      ",\n",
      "as\n",
      "generative\n",
      "models\n",
      "have\n",
      "become\n",
      "increasingly\n",
      "popular\n",
      ",\n",
      "several\n",
      "studies\n",
      "have\n",
      "proposed\n",
      "to\n",
      "probe\n",
      "political\n",
      "biases\n",
      "(\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Jiang\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022b\n",
      ")\n",
      "and\n",
      "prudence\n",
      "(\n",
      "Bang\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      "of\n",
      "these\n",
      "models\n",
      ".\n",
      "Liu\n",
      "et\n",
      "al\n",
      ".\n",
      "(\n",
      "2021\n",
      ")\n",
      "presented\n",
      "two\n",
      "metrics\n",
      "to\n",
      "quantify\n",
      "political\n",
      "bias\n",
      "in\n",
      "GPT2\n",
      "using\n",
      "a\n",
      "political\n",
      "ideology\n",
      "classifier\n",
      ",\n",
      "which\n",
      "evaluate\n",
      "the\n",
      "probability\n",
      "difference\n",
      "of\n",
      "generated\n",
      "text\n",
      "with\n",
      "and\n",
      "without\n",
      "attributes\n",
      "(\n",
      "gender\n",
      ",\n",
      "location\n",
      ",\n",
      "and\n",
      "topic\n",
      ")\n",
      ".\n",
      "Jiang\n",
      "et\n",
      "al\n",
      ".\n",
      "(\n",
      "2022b\n",
      ")\n",
      "showed\n",
      "that\n",
      "LMs\n",
      "trained\n",
      "on\n",
      "corpora\n",
      "written\n",
      "by\n",
      "active\n",
      "partisan\n",
      "members\n",
      "of\n",
      "a\n",
      "community\n",
      "can\n",
      "be\n",
      "used\n",
      "to\n",
      "examine\n",
      "the\n",
      "perspective\n",
      "of\n",
      "the\n",
      "community\n",
      "and\n",
      "generate\n",
      "community\n",
      "-\n",
      "specific\n",
      "responses\n",
      "to\n",
      "elicit\n",
      "opinions\n",
      "about\n",
      "political\n",
      "entities\n",
      ".\n",
      "Our\n",
      "proposed\n",
      "method\n",
      "is\n",
      "distinct\n",
      "from\n",
      "existing\n",
      "methods\n",
      "as\n",
      "it\n",
      "can\n",
      "be\n",
      "applied\n",
      "to\n",
      "a\n",
      "wide\n",
      "range\n",
      "of\n",
      "LMs\n",
      "including\n",
      "encoder\n",
      "-\n",
      "based\n",
      "models\n",
      ",\n",
      "not\n",
      "just\n",
      "autoregressive\n",
      "models\n",
      ".\n",
      "Additionally\n",
      ",\n",
      "our\n",
      "approach\n",
      "for\n",
      "measuring\n",
      "political\n",
      "bias\n",
      "is\n",
      "informed\n",
      "by\n",
      "existing\n",
      "political\n",
      "science\n",
      "literature\n",
      "and\n",
      "widely\n",
      "-\n",
      "used\n",
      "standard\n",
      "tests\n",
      ".\n",
      "Impact\n",
      "of\n",
      "Model\n",
      "and\n",
      "Data\n",
      "Bias\n",
      "on\n",
      "Downstream\n",
      "Task\n",
      "Fairness\n",
      "Previous\n",
      "research\n",
      "has\n",
      "shown\n",
      "that\n",
      "the\n",
      "performance\n",
      "of\n",
      "models\n",
      "for\n",
      "downstream\n",
      "tasks\n",
      "can\n",
      "vary\n",
      "greatly\n",
      "among\n",
      "different\n",
      "identity\n",
      "groups\n",
      "(\n",
      "Hovy\n",
      "and\n",
      "Søgaard\n",
      ",\n",
      "2015\n",
      ";\n",
      "Buolamwini\n",
      "and\n",
      "Gebru\n",
      ",\n",
      "2018\n",
      ";\n",
      "Dixon\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ")\n",
      ",\n",
      "highlighting\n",
      "the\n",
      "issue\n",
      "of\n",
      "fairness\n",
      "(\n",
      "Hutchinson\n",
      "and\n",
      "Mitchell\n",
      ",\n",
      "2019\n",
      ";\n",
      ".\n",
      "It\n",
      "is\n",
      "commonly\n",
      "believed\n",
      "that\n",
      "annotator\n",
      "(\n",
      "Geva\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ";\n",
      "Sap\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ";\n",
      "Davani\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ";\n",
      "Sap\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      "and\n",
      "data\n",
      "bias\n",
      "(\n",
      "Park\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ";\n",
      "Dixon\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2018\n",
      ";\n",
      "Dodge\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ";\n",
      "Harris\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2022\n",
      ")\n",
      "are\n",
      "the\n",
      "cause\n",
      "of\n",
      "this\n",
      "impact\n",
      ",\n",
      "and\n",
      "some\n",
      "studies\n",
      "have\n",
      "investigated\n",
      "the\n",
      "connection\n",
      "between\n",
      "training\n",
      "data\n",
      "and\n",
      "downstream\n",
      "task\n",
      "model\n",
      "behavior\n",
      "(\n",
      "Gonen\n",
      "and\n",
      "Webster\n",
      ",\n",
      "2020\n",
      ";\n",
      "Dodge\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2021\n",
      ")\n",
      ".\n",
      "Our\n",
      "study\n",
      "adds\n",
      "to\n",
      "this\n",
      "by\n",
      "demonstrating\n",
      "the\n",
      "effects\n",
      "of\n",
      "political\n",
      "bias\n",
      "in\n",
      "training\n",
      "data\n",
      "on\n",
      "downstream\n",
      "tasks\n",
      ",\n",
      "specifically\n",
      "in\n",
      "terms\n",
      "of\n",
      "fairness\n",
      ".\n",
      "Previous\n",
      "studies\n",
      "have\n",
      "primarily\n",
      "examined\n",
      "the\n",
      "connection\n",
      "between\n",
      "data\n",
      "bias\n",
      "and\n",
      "either\n",
      "model\n",
      "bias\n",
      "or\n",
      "downstream\n",
      "task\n",
      "performance\n",
      ",\n",
      "with\n",
      "the\n",
      "exception\n",
      "of\n",
      "Steed\n",
      "et\n",
      "al\n",
      ".\n",
      "(\n",
      "2022\n",
      ")\n",
      ".\n",
      "Our\n",
      "study\n",
      ",\n",
      "however\n",
      ",\n",
      "takes\n",
      "a\n",
      "more\n",
      "thorough\n",
      "approach\n",
      "by\n",
      "linking\n",
      "data\n",
      "bias\n",
      "to\n",
      "model\n",
      "bias\n",
      ",\n",
      "and\n",
      "then\n",
      "to\n",
      "downstream\n",
      "task\n",
      "performance\n",
      ",\n",
      "in\n",
      "order\n",
      "to\n",
      "gain\n",
      "a\n",
      "more\n",
      "complete\n",
      "understanding\n",
      "of\n",
      "the\n",
      "effect\n",
      "of\n",
      "social\n",
      "biases\n",
      "on\n",
      "the\n",
      "fairness\n",
      "of\n",
      "models\n",
      "for\n",
      "downstream\n",
      "tasks\n",
      ".\n",
      "Also\n",
      ",\n",
      "most\n",
      "prior\n",
      "work\n",
      "has\n",
      "primarily\n",
      "focused\n",
      "on\n",
      "investigating\n",
      "fairness\n",
      "in\n",
      "hate\n",
      "speech\n",
      "detection\n",
      "models\n",
      ",\n",
      "but\n",
      "our\n",
      "study\n",
      "highlights\n",
      "important\n",
      "fairness\n",
      "concerns\n",
      "in\n",
      "misinformation\n",
      "detection\n",
      "that\n",
      "require\n",
      "further\n",
      "examination\n",
      ".\n",
      "Conclusion\n",
      "We\n",
      "conduct\n",
      "a\n",
      "systematic\n",
      "analysis\n",
      "of\n",
      "the\n",
      "political\n",
      "biases\n",
      "of\n",
      "language\n",
      "models\n",
      ".\n",
      "We\n",
      "probe\n",
      "LMs\n",
      "using\n",
      "prompts\n",
      "grounded\n",
      "in\n",
      "political\n",
      "science\n",
      "and\n",
      "measure\n",
      "models\n",
      "'\n",
      "ideological\n",
      "positions\n",
      "on\n",
      "social\n",
      "and\n",
      "economic\n",
      "values\n",
      ".\n",
      "We\n",
      "also\n",
      "examine\n",
      "the\n",
      "influence\n",
      "of\n",
      "political\n",
      "biases\n",
      "in\n",
      "pretraining\n",
      "data\n",
      "on\n",
      "the\n",
      "political\n",
      "leanings\n",
      "of\n",
      "LMs\n",
      "and\n",
      "investigate\n",
      "the\n",
      "model\n",
      "performance\n",
      "with\n",
      "varying\n",
      "political\n",
      "biases\n",
      "on\n",
      "downstream\n",
      "tasks\n",
      ",\n",
      "finding\n",
      "that\n",
      "LMs\n",
      "may\n",
      "have\n",
      "different\n",
      "standards\n",
      "for\n",
      "different\n",
      "hate\n",
      "speech\n",
      "targets\n",
      "and\n",
      "misinformation\n",
      "sources\n",
      "based\n",
      "on\n",
      "their\n",
      "political\n",
      "biases\n",
      ".\n",
      "Our\n",
      "work\n",
      "highlights\n",
      "that\n",
      "pernicious\n",
      "biases\n",
      "and\n",
      "unfairness\n",
      "in\n",
      "downstream\n",
      "tasks\n",
      "can\n",
      "be\n",
      "caused\n",
      "by\n",
      "non\n",
      "-\n",
      "toxic\n",
      "data\n",
      ",\n",
      "which\n",
      "includes\n",
      "diverse\n",
      "opinions\n",
      ",\n",
      "but\n",
      "there\n",
      "are\n",
      "subtle\n",
      "imbalances\n",
      "in\n",
      "data\n",
      "distributions\n",
      ".\n",
      "Prior\n",
      "work\n",
      "discussed\n",
      "data\n",
      "filtering\n",
      "or\n",
      "augmentation\n",
      "techniques\n",
      "as\n",
      "a\n",
      "remedy\n",
      "(\n",
      "Kaushik\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ";\n",
      "while\n",
      "useful\n",
      "in\n",
      "theory\n",
      ",\n",
      "these\n",
      "approaches\n",
      "might\n",
      "not\n",
      "be\n",
      "applicable\n",
      "in\n",
      "real\n",
      "-\n",
      "world\n",
      "settings\n",
      ",\n",
      "running\n",
      "the\n",
      "risk\n",
      "of\n",
      "censorship\n",
      "and\n",
      "exclusion\n",
      "from\n",
      "political\n",
      "participation\n",
      ".\n",
      "In\n",
      "addition\n",
      "to\n",
      "identifying\n",
      "these\n",
      "risks\n",
      ",\n",
      "we\n",
      "discuss\n",
      "strategies\n",
      "to\n",
      "mitigate\n",
      "the\n",
      "negative\n",
      "impacts\n",
      "while\n",
      "preserving\n",
      "the\n",
      "diversity\n",
      "of\n",
      "opinions\n",
      "in\n",
      "pretraining\n",
      "data\n",
      ".\n",
      "Limitations\n",
      "The\n",
      "Political\n",
      "Compass\n",
      "Test\n",
      "In\n",
      "this\n",
      "work\n",
      ",\n",
      "we\n",
      "leveraged\n",
      "the\n",
      "political\n",
      "compass\n",
      "test\n",
      "as\n",
      "a\n",
      "test\n",
      "bed\n",
      "to\n",
      "probe\n",
      "the\n",
      "underlying\n",
      "political\n",
      "leaning\n",
      "of\n",
      "pretrained\n",
      "language\n",
      "models\n",
      ".\n",
      "While\n",
      "the\n",
      "political\n",
      "compass\n",
      "test\n",
      "is\n",
      "a\n",
      "widely\n",
      "adopted\n",
      "and\n",
      "straightforward\n",
      "toolkit\n",
      ",\n",
      "it\n",
      "is\n",
      "far\n",
      "from\n",
      "perfect\n",
      "and\n",
      "has\n",
      "several\n",
      "limitations\n",
      ":\n",
      "1\n",
      ")\n",
      "In\n",
      "addition\n",
      "to\n",
      "a\n",
      "two\n",
      "-\n",
      "axis\n",
      "political\n",
      "spectrum\n",
      "on\n",
      "social\n",
      "and\n",
      "economic\n",
      "values\n",
      "(\n",
      "Eysenck\n",
      ",\n",
      "1957\n",
      ")\n",
      ",\n",
      "there\n",
      "are\n",
      "numerous\n",
      "political\n",
      "science\n",
      "theories\n",
      "(\n",
      "Blattberg\n",
      ",\n",
      "2001\n",
      ";\n",
      "Horrell\n",
      ",\n",
      "2005\n",
      ";\n",
      "Diamond\n",
      "and\n",
      "Wolf\n",
      ",\n",
      "2017\n",
      ")\n",
      "that\n",
      "support\n",
      "other\n",
      "ways\n",
      "of\n",
      "categorizing\n",
      "political\n",
      "ideologies\n",
      ".\n",
      "2\n",
      ")\n",
      "The\n",
      "political\n",
      "compass\n",
      "test\n",
      "focuses\n",
      "heavily\n",
      "on\n",
      "the\n",
      "ideological\n",
      "issues\n",
      "and\n",
      "debates\n",
      "of\n",
      "the\n",
      "western\n",
      "world\n",
      ",\n",
      "while\n",
      "the\n",
      "political\n",
      "landscape\n",
      "is\n",
      "far\n",
      "from\n",
      "homogeneous\n",
      "around\n",
      "the\n",
      "globe\n",
      ".\n",
      "(\n",
      "Hudson\n",
      ",\n",
      "1978\n",
      ")\n",
      "3\n",
      ")\n",
      "There\n",
      "are\n",
      "several\n",
      "criticisms\n",
      "of\n",
      "the\n",
      "political\n",
      "compass\n",
      "test\n",
      ":\n",
      "unclear\n",
      "scoring\n",
      "schema\n",
      ",\n",
      "libertarian\n",
      "bias\n",
      ",\n",
      "and\n",
      "vague\n",
      "statement\n",
      "formulation\n",
      "(\n",
      "Utley\n",
      ",\n",
      "2001\n",
      ";\n",
      "Mitchell\n",
      ",\n",
      "2007\n",
      ")\n",
      ".\n",
      "However\n",
      ",\n",
      "we\n",
      "present\n",
      "a\n",
      "general\n",
      "methodology\n",
      "to\n",
      "probe\n",
      "the\n",
      "political\n",
      "leaning\n",
      "of\n",
      "LMs\n",
      "that\n",
      "is\n",
      "compatible\n",
      "with\n",
      "any\n",
      "ideological\n",
      "theories\n",
      ",\n",
      "tests\n",
      ",\n",
      "and\n",
      "questionnaires\n",
      ".\n",
      "We\n",
      "encourage\n",
      "readers\n",
      "to\n",
      "use\n",
      "our\n",
      "approach\n",
      "along\n",
      "with\n",
      "other\n",
      "ideological\n",
      "theories\n",
      "and\n",
      "tests\n",
      "for\n",
      "a\n",
      "more\n",
      "well\n",
      "-\n",
      "rounded\n",
      "evaluation\n",
      ".\n",
      "Probing\n",
      "Language\n",
      "Models\n",
      "For\n",
      "encoder\n",
      "-\n",
      "based\n",
      "language\n",
      "models\n",
      ",\n",
      "our\n",
      "approach\n",
      "of\n",
      "mask\n",
      "in\n",
      "-\n",
      "filling\n",
      "is\n",
      "widely\n",
      "adopted\n",
      "in\n",
      "numerous\n",
      "existing\n",
      "works\n",
      "(\n",
      "Petroni\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ";\n",
      ".\n",
      "For\n",
      "language\n",
      "generation\n",
      "models\n",
      ",\n",
      "we\n",
      "curate\n",
      "prompts\n",
      ",\n",
      "conduct\n",
      "prompted\n",
      "text\n",
      "generation\n",
      ",\n",
      "and\n",
      "employ\n",
      "a\n",
      "BARTbased\n",
      "stance\n",
      "detector\n",
      "for\n",
      "response\n",
      "evaluation\n",
      ".\n",
      "An\n",
      "alternative\n",
      "approach\n",
      "would\n",
      "be\n",
      "to\n",
      "explicitly\n",
      "frame\n",
      "it\n",
      "as\n",
      "a\n",
      "multi\n",
      "-\n",
      "choice\n",
      "question\n",
      "in\n",
      "the\n",
      "prompt\n",
      ",\n",
      "forcing\n",
      "pretrained\n",
      "language\n",
      "models\n",
      "to\n",
      "choose\n",
      "from\n",
      "STRONG\n",
      "AGREE\n",
      ",\n",
      "AGREE\n",
      ",\n",
      "DISAGREE\n",
      ",\n",
      "and\n",
      "STRONG\n",
      "DISAGREE\n",
      ".\n",
      "These\n",
      "two\n",
      "approaches\n",
      "have\n",
      "their\n",
      "respective\n",
      "pros\n",
      "and\n",
      "cons\n",
      ":\n",
      "our\n",
      "approach\n",
      "is\n",
      "compatible\n",
      "with\n",
      "all\n",
      "LMs\n",
      "that\n",
      "support\n",
      "text\n",
      "generation\n",
      "and\n",
      "is\n",
      "more\n",
      "interpretable\n",
      ",\n",
      "while\n",
      "the\n",
      "response\n",
      "mapping\n",
      "and\n",
      "the\n",
      "stance\n",
      "detector\n",
      "could\n",
      "be\n",
      "more\n",
      "subjective\n",
      "and\n",
      "rely\n",
      "on\n",
      "empirical\n",
      "hyperparameter\n",
      "settings\n",
      ";\n",
      "multichoice\n",
      "questions\n",
      "offer\n",
      "direct\n",
      "and\n",
      "unequivocal\n",
      "answers\n",
      ",\n",
      "while\n",
      "being\n",
      "less\n",
      "interpretable\n",
      "and\n",
      "does\n",
      "not\n",
      "work\n",
      "well\n",
      "with\n",
      "LMs\n",
      "with\n",
      "fewer\n",
      "parameters\n",
      "such\n",
      "as\n",
      "GPT-2\n",
      "(\n",
      "Radford\n",
      "et\n",
      "al\n",
      ".\n",
      ",\n",
      "2019\n",
      ")\n",
      ".\n",
      "Fine\n",
      "-\n",
      "Grained\n",
      "Political\n",
      "Leaning\n",
      "Analysis\n",
      "In\n",
      "this\n",
      "work\n",
      ",\n",
      "we\n",
      "\"\n",
      "force\n",
      "\"\n",
      "each\n",
      "pretrained\n",
      "LM\n",
      "into\n",
      "its\n",
      "position\n",
      "on\n",
      "a\n",
      "two\n",
      "-\n",
      "dimensional\n",
      "space\n",
      "based\n",
      "on\n",
      "their\n",
      "responses\n",
      "to\n",
      "social\n",
      "and\n",
      "economic\n",
      "issues\n",
      ".\n",
      "However\n",
      ",\n",
      "political\n",
      "leaning\n",
      "could\n",
      "be\n",
      "more\n",
      "fine\n",
      "-\n",
      "grained\n",
      "than\n",
      "two\n",
      "numerical\n",
      "values\n",
      ":\n",
      "being\n",
      "liberal\n",
      "on\n",
      "one\n",
      "issue\n",
      "does\n",
      "not\n",
      "necessarily\n",
      "exclude\n",
      "the\n",
      "possibility\n",
      "of\n",
      "being\n",
      "conservative\n",
      "on\n",
      "another\n",
      ",\n",
      "and\n",
      "vice\n",
      "versa\n",
      ".\n",
      "We\n",
      "leave\n",
      "it\n",
      "to\n",
      "future\n",
      "work\n",
      "on\n",
      "how\n",
      "to\n",
      "achieve\n",
      "a\n",
      "more\n",
      "fine\n",
      "-\n",
      "grained\n",
      "understanding\n",
      "of\n",
      "LM\n",
      "political\n",
      "leaning\n",
      "in\n",
      "a\n",
      "topic\n",
      "-\n",
      "and\n",
      "issue\n",
      "-\n",
      "specific\n",
      "manner\n",
      ".\n",
      "Ethics\n",
      "Statement\n",
      "U.S.-Centric\n",
      "Perspectives\n",
      "The\n",
      "authors\n",
      "of\n",
      "this\n",
      "work\n",
      "are\n",
      "based\n",
      "in\n",
      "the\n",
      "U.S.\n",
      ",\n",
      "and\n",
      "our\n",
      "framing\n",
      "in\n",
      "this\n",
      "work\n",
      ",\n",
      "e.g.\n",
      ",\n",
      "references\n",
      "to\n",
      "minority\n",
      "identity\n",
      "groups\n",
      ",\n",
      "reflects\n",
      "this\n",
      "context\n",
      ".\n",
      "This\n",
      "viewpoint\n",
      "is\n",
      "not\n",
      "universally\n",
      "applicable\n",
      "and\n",
      "may\n",
      "vary\n",
      "in\n",
      "different\n",
      "contexts\n",
      "and\n",
      "cultures\n",
      ".\n",
      "Misuse\n",
      "Potential\n",
      "In\n",
      "this\n",
      "paper\n",
      ",\n",
      "we\n",
      "showed\n",
      "that\n",
      "hyperpartisan\n",
      "LMs\n",
      "are\n",
      "not\n",
      "simply\n",
      "achieved\n",
      "by\n",
      "pretraining\n",
      "on\n",
      "more\n",
      "partisan\n",
      "data\n",
      "for\n",
      "more\n",
      "epochs\n",
      ".\n",
      "However\n",
      ",\n",
      "this\n",
      "preliminary\n",
      "finding\n",
      "does\n",
      "not\n",
      "exclude\n",
      "the\n",
      "possibility\n",
      "of\n",
      "future\n",
      "malicious\n",
      "attempts\n",
      "at\n",
      "creating\n",
      "hyperpartisan\n",
      "language\n",
      "models\n",
      ",\n",
      "and\n",
      "some\n",
      "might\n",
      "even\n",
      "succeed\n",
      ".\n",
      "Training\n",
      "and\n",
      "employing\n",
      "hyperpartisan\n",
      "LMs\n",
      "might\n",
      "contribute\n",
      "to\n",
      "many\n",
      "malicious\n",
      "purposes\n",
      ",\n",
      "such\n",
      "as\n",
      "propagating\n",
      "partisan\n",
      "misinformation\n",
      "or\n",
      "adversarially\n",
      "attacking\n",
      "pretrained\n",
      "language\n",
      "models\n",
      "(\n",
      "Bagdasaryan\n",
      "and\n",
      "Shmatikov\n",
      ",\n",
      "2022\n",
      ")\n",
      ".\n",
      "We\n",
      "will\n",
      "refrain\n",
      "from\n",
      "releasing\n",
      "the\n",
      "trained\n",
      "hyperpartisan\n",
      "language\n",
      "model\n",
      "checkpoints\n",
      "and\n",
      "will\n",
      "establish\n",
      "access\n",
      "permission\n",
      "for\n",
      "the\n",
      "collected\n",
      "partisan\n",
      "pretraining\n",
      "corpora\n",
      "to\n",
      "ensure\n",
      "its\n",
      "research\n",
      "-\n",
      "only\n",
      "usage\n",
      ".\n",
      "Interpreting\n",
      "Downstream\n",
      "Task\n",
      "Performance\n",
      "While\n",
      "we\n",
      "showed\n",
      "that\n",
      "pretrained\n",
      "LMs\n",
      "with\n",
      "different\n",
      "political\n",
      "leanings\n",
      "could\n",
      "have\n",
      "differen\n"
     ]
    }
   ],
   "source": [
    "file_path = \"output.conll\"\n",
    "with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.strip().split()\n",
    "            if len(split_line) > 1:    \n",
    "                print(split_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokens', 'ner_tags']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokens', 'ner_tags']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
